# Interaction et recherche

Ce document d√©taille les fonctionnalit√©s d'interaction avec les contrats, notamment la recherche s√©mantique et le mode chat.

## Vue d'ensemble

Contract Management RAG offre deux modes principaux d'interaction avec les contrats :

1. **Recherche s√©mantique** - Pour trouver rapidement des passages pertinents
2. **Chat avec les contrats** - Pour poser des questions en langage naturel et obtenir des r√©ponses contextualis√©es

Ces deux modes sont impl√©ment√©s dans le module `core/interaction.py` et accessibles via l'interface de ligne de commande `main.py`.

## 1. Recherche s√©mantique

### Principe de fonctionnement

La recherche s√©mantique permet de trouver des passages pertinents m√™me si les termes exacts de la requ√™te ne sont pas pr√©sents dans le texte. Elle fonctionne selon ce processus :

1. La requ√™te de l'utilisateur est convertie en embedding vectoriel
2. Ce vecteur est compar√© √† tous les embeddings des chunks stock√©s dans ChromaDB
3. Les chunks les plus similaires (distance vectorielle la plus faible) sont retourn√©s

### Impl√©mentation

La recherche est impl√©ment√©e dans la fonction `display_contract_search_results` du module `core/interaction.py` :

```python
def display_contract_search_results(query: str, n_results: int = 5) -> None:
    """
    Search in the contract database

    Args:
        query: Search query
        n_results: Number of results to return
    """
    logger.info(f"\nüîç Recherche: {query}")

    # Initialize managers
    embeddings_manager = TextVectorizer()
    chroma_manager = VectorDBInterface(embeddings_manager)

    # Search
    results = chroma_manager.search(query, n_results=n_results)

    # Display results
    logger.info(f"\nüìä R√©sultats ({len(results)} trouv√©s):")
    for i, result in enumerate(results, 1):
        logger.info(f"\n--- R√©sultat {i} ---")
        logger.info(f"Section: {result['metadata']['section']}")
        logger.info(f"Hi√©rarchie: {result['metadata']['hierarchy']}")
        logger.info(f"Document: {result['metadata']['document_title']}")
        logger.info(f"Contenu: {result['document'][:200]}...")
        logger.info(f"Distance: {result['distance']:.4f}")
```

### Exemple d'utilisation

```bash
python main.py data/contrat_prestation.pdf "quelles sont les modalit√©s de paiement"
```

### Format des r√©sultats

Chaque r√©sultat contient :
- **Section** : Le num√©ro ou identifiant de la section concern√©e
- **Hi√©rarchie** : La position dans la structure hi√©rarchique du document
- **Document** : Le titre du document source
- **Contenu** : Le texte pertinent (limit√© aux 200 premiers caract√®res dans l'affichage)
- **Distance** : La mesure de similarit√© (plus elle est proche de 0, plus le r√©sultat est pertinent)

### Optimisations et param√®tres

La recherche peut √™tre personnalis√©e via plusieurs param√®tres :

- **Nombre de r√©sultats** : Configurable via le param√®tre `n_results`
- **Filtrage par m√©tadonn√©es** : Possibilit√© de filtrer par document, section, etc.
- **Seuils de pertinence** : Les r√©sultats avec une distance > 0.6 sont g√©n√©ralement peu pertinents

## 2. Chat avec les contrats

### Principe de fonctionnement

Le mode chat permet d'interagir avec les contrats en langage naturel en suivant ces √©tapes :

1. La question de l'utilisateur est vectoris√©e et compar√©e aux chunks du contrat
2. Les chunks les plus pertinents sont r√©cup√©r√©s pour former le contexte
3. Ce contexte et la question sont envoy√©s √† un mod√®le de langage (LLM)
4. Le LLM g√©n√®re une r√©ponse en utilisant uniquement les informations du contexte
5. La r√©ponse et les sources sont pr√©sent√©es √† l'utilisateur

### Impl√©mentation

Le chat est impl√©ment√© dans la fonction `chat_with_contract` du module `core/interaction.py` :

```python
def chat_with_contract(query: str, n_context: int = 3) -> None:
    """
    Chat with the contract using embeddings for context and Ollama for generation

    Args:
        query: User's question
        n_context: Number of relevant chunks to use as context
    """
    logger.info(f"\nüí¨ Chat: {query}")

    # Initialize managers
    embeddings_manager = TextVectorizer()
    chroma_manager = VectorDBInterface(embeddings_manager)

    # Search for relevant context
    results = chroma_manager.search(query, n_results=n_context)

    # Prepare context for the prompt
    context = "\n\n".join(
        [
            f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}\n"
            f"Section: {result['metadata'].get('section_number', 'Non sp√©cifi√©')}\n"
            f"Chapter: {result['metadata'].get('chapter_title', 'Non sp√©cifi√©')}\n"
            f"Content: {result['document']}"
            for result in results
        ]
    )

    # Create the prompt with context
    prompt = f"""Tu es un assistant sp√©cialis√© dans l'analyse de contrats. 
Voici le contexte pertinent extrait des documents :

{context}

Question de l'utilisateur : {query}

R√©ponds de mani√®re pr√©cise en te basant uniquement sur le contexte fourni. 
Si tu ne trouves pas l'information dans le contexte, dis-le clairement."""

    # Get response from Ollama
    response = llm_chat_call_with_ollama(prompt)
    logger.info("\nü§ñ R√©ponse :")
    logger.info(response)

    # Display sources with metadata
    logger.info("\nüìö Sources :")
    logger.info("=" * 80)
    for i, result in enumerate(results, 1):
        logger.info("\n" + "-" * 40)
        logger.info(f"\nSource {i}/{len(results)}")
        logger.info("-" * 40)

        logger.info(f"Distance: {result['distance']:.4f}")

        # Afficher le contenu
        logger.info(result["metadata"].get("content", result["document"])[:200] + "...")
        logger.info("-" * 40)

    logger.info(f"\nüìä Nombre total de sources: {len(results)}")
```

### Mod√®le de langage (LLM)

Le syst√®me utilise Ollama pour ex√©cuter un mod√®le de langage localement. Le mod√®le recommand√© est `mistral-small3.1`, mais d'autres mod√®les peuvent √™tre configur√©s.

Le module `document_processing/llm_chat.py` g√®re l'interaction avec Ollama :

```python
def llm_chat_call_with_ollama(prompt: str) -> str:
    """
    Ask a question to Ollama LLM

    Args:
        prompt: The prompt to send to Ollama

    Returns:
        The response from Ollama
    """
    # Configuration
    model = os.getenv("LLM_MODEL", "mistral-small3.1")
    ollama_host = os.getenv("LLM_HOST", "http://localhost:11434")
    
    # Prepare request
    url = f"{ollama_host}/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.1,
            "top_p": 0.95,
            "top_k": 40
        }
    }
    
    # Send request
    try:
        response = requests.post(url, json=data)
        response.raise_for_status()
        return response.json()["response"]
    except Exception as e:
        logger.error(f"Erreur lors de la communication avec Ollama: {e}")
        return "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. V√©rifiez qu'Ollama est bien en cours d'ex√©cution."
```

### Exemple d'utilisation interactive

```bash
python main.py data/contrat_prestation.pdf --chat

üí¨ Mode chat activ√©. Tapez 'exit' pour quitter.

Votre question : Quelles sont les obligations du prestataire?
```

### Personnalisation du chat

Plusieurs aspects du chat peuvent √™tre personnalis√©s :

- **Nombre de chunks de contexte** : Via le param√®tre `n_context` (3 par d√©faut)
- **Temp√©rature du LLM** : Contr√¥le la cr√©ativit√© des r√©ponses (0.1 par d√©faut pour des r√©ponses factuelles)
- **Mod√®le utilis√©** : Configurable via la variable d'environnement `LLM_MODEL`
- **Prompt template** : Personnalisable pour adapter le comportement du LLM

### Limitations et bonnes pratiques

- **Questions sp√©cifiques** : Les questions pr√©cises obtiennent g√©n√©ralement de meilleures r√©ponses
- **Contexte limit√©** : Le syst√®me n'utilise que les chunks les plus pertinents (par d√©faut 3), ce qui peut limiter la vision globale
- **Sources v√©rifiables** : V√©rifiez toujours les sources pr√©sent√©es pour confirmer la r√©ponse
- **Limite de connaissance** : Le syst√®me ne conna√Æt que ce qui est pr√©sent dans les contrats analys√©s

## 3. Fonctionnalit√©s avanc√©es

### Filtrage par m√©tadonn√©es

Il est possible de filtrer les recherches par m√©tadonn√©es, par exemple pour se concentrer sur certaines sections ou documents :

```python
# Exemple : Filtrage par section
results = chroma_manager.search(
    query, 
    n_results=n_results,
    filter={"section_number": {"$in": ["3", "3.1", "3.2"]}}
)

# Exemple : Filtrage par document
results = chroma_manager.search(
    query, 
    n_results=n_results,
    filter={"document_title": "Contrat de Prestation de Services"}
)
```

### Contextualisation des r√©ponses

Le syst√®me enrichit les r√©ponses avec des informations contextuelles :

- Position hi√©rarchique exacte dans le document
- Num√©ros de sections et d'articles
- R√©f√©rences √† d'autres clauses li√©es
- Citations pr√©cises du texte original

### Mode d'explication

Une fonctionnalit√© exp√©rimentale permet de demander des explications sur des clauses complexes :

```bash
python main.py data/contrat_complexe.pdf --chat

Votre question : Explique en termes simples la clause 5.3 sur la limitation de responsabilit√©

ü§ñ R√©ponse :
La clause 5.3 explique essentiellement que l'entreprise ne peut pas √™tre tenue responsable pour plus que le montant que vous avez pay√© pour ses services...
```

## 4. Extension et personnalisation

### Ajout de nouveaux mod√®les LLM

Le syst√®me peut √™tre √©tendu pour utiliser d'autres mod√®les LLM :

1. Modifier le module `document_processing/llm_chat.py` pour ajouter un nouveau connecteur
2. Configurer le nouveau mod√®le dans `config.env`

### Personnalisation des prompts

Les prompts envoy√©s au LLM peuvent √™tre personnalis√©s pour diff√©rents cas d'usage :

- Analyse juridique approfondie
- Extraction d'informations sp√©cifiques (dates, montants, parties)
- Comparaison entre diff√©rentes clauses

### Int√©gration d'API externes

Le syst√®me peut √™tre √©tendu pour int√©grer des API externes :

- Bases de donn√©es juridiques
- Services de traduction
- Services d'analyse suppl√©mentaires 