#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script pour t√©l√©charger le mod√®le nomic-ai/nomic-embed-text-v2-moe pour utilisation hors ligne.
"""

import os
import sys
import logging
from pathlib import Path

# Ajouter le r√©pertoire parent au PYTHONPATH pour importer les modules du projet
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from dotenv import load_dotenv
import torch
from transformers import AutoModel, AutoTokenizer

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("./logs/download_nomic_model.log", mode="w"),
    ],
)
logger = logging.getLogger("download_nomic_model")

def ensure_directory_exists(directory):
    """S'assure qu'un r√©pertoire existe, le cr√©e sinon"""
    Path(directory).mkdir(parents=True, exist_ok=True)
    logger.info(f"‚úÖ R√©pertoire v√©rifi√©: {directory}")

def download_nomic_model():
    """T√©l√©charge le mod√®le nomic-ai/nomic-embed-text-v2-moe pour utilisation hors ligne"""
    # Charger les variables d'environnement
    load_dotenv("config.env")
    
    # Obtenir le r√©pertoire de mod√®les √† partir de config.env
    models_dir = Path(os.getenv("MODELS_DIR", "offline_models/hf"))
    ensure_directory_exists(models_dir)
    
    # Mod√®le √† t√©l√©charger
    model_name = "nomic-ai/nomic-embed-text-v2-moe"
    
    logger.info(f"üîÑ T√©l√©chargement du mod√®le {model_name}...")
    logger.info(f"Destination: {models_dir}")
    
    try:
        # D√©finir le dossier cache pour HuggingFace
        os.environ["TRANSFORMERS_CACHE"] = str(models_dir)
        
        # T√©l√©charger le tokenizer
        logger.info(f"T√©l√©chargement du tokenizer pour {model_name}...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # T√©l√©charger le mod√®le
        logger.info(f"T√©l√©chargement du mod√®le {model_name}...")
        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        
        # Forcer un calcul d'embedding simple pour v√©rifier que tout fonctionne
        logger.info("Test du mod√®le t√©l√©charg√©...")
        test_text = "Ceci est un test pour v√©rifier que le mod√®le fonctionne correctement."
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        
        # Encoder le texte
        inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True)
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            # V√©rifiez que les embeddings sont bien produits
            if hasattr(outputs, "last_hidden_state"):
                logger.info(f"‚úÖ Test r√©ussi - Dimension des embeddings: {outputs.last_hidden_state.shape}")
            else:
                logger.info(f"‚úÖ Test r√©ussi - Output: {type(outputs)}")
        
        # Sauvegarder le mod√®le et le tokenizer
        logger.info(f"Sauvegarde du mod√®le et du tokenizer dans {models_dir}...")
        model_save_path = models_dir / model_name.split("/")[-1]
        tokenizer.save_pretrained(model_save_path)
        model.save_pretrained(model_save_path)
        
        logger.info(f"‚úÖ Mod√®le {model_name} t√©l√©charg√© et sauvegard√© avec succ√®s dans {model_save_path}")
        
        # Ajouter √† config.env si n√©cessaire
        update_embedding_model = input("Voulez-vous d√©finir ce mod√®le comme mod√®le d'embedding par d√©faut dans config.env? (o/n): ").lower()
        if update_embedding_model == 'o' or update_embedding_model == 'oui':
            config_file = Path("config.env")
            if config_file.exists():
                with open(config_file, "r") as f:
                    config_lines = f.readlines()
                
                # Chercher et mettre √† jour la variable EMBEDDING_MODEL
                found = False
                for i, line in enumerate(config_lines):
                    if line.strip().startswith("EMBEDDING_MODEL="):
                        config_lines[i] = f'EMBEDDING_MODEL="{model_name}"\n'
                        found = True
                        break
                
                # Ajouter la variable si elle n'existe pas
                if not found:
                    config_lines.append(f'EMBEDDING_MODEL="{model_name}"\n')
                
                # √âcrire le fichier mis √† jour
                with open(config_file, "w") as f:
                    f.writelines(config_lines)
                
                logger.info(f"‚úÖ Fichier config.env mis √† jour avec EMBEDDING_MODEL={model_name}")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du t√©l√©chargement du mod√®le {model_name}: {str(e)}")
        return False

if __name__ == "__main__":
    # S'assurer que le r√©pertoire des logs existe
    ensure_directory_exists("logs")
    
    # T√©l√©charger le mod√®le
    success = download_nomic_model()
    
    if success:
        logger.info("‚úÖ Op√©ration termin√©e avec succ√®s")
        sys.exit(0)
    else:
        logger.error("‚ùå Op√©ration termin√©e avec erreurs")
        sys.exit(1) 