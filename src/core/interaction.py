from document_processing.llm_chat import llm_chat_call_with_ollama
from document_processing.text_vectorizer import TextVectorizer
from document_processing.vectordb_interface import VectorDBInterface
from core.graph_manager import GraphManager
from document_processing.reranker import Reranker
from core.subquery_divider import define_subqueries
from core.alternative_query_creator import generate_queries
from utils.logger import setup_logger
from utils.hybrid_search import HybridSearch
import os
from dotenv import load_dotenv
# Import the query classification
from core.query_classification_v2 import determine_inference_mode
from core.history_func import retrieve_similar_conversations, add_conversation_in_history_db, get_history

# Configurer le logger pour ce module
logger = setup_logger(__file__)
load_dotenv("config.env")

# Variable globale pour stocker l'instance de recherche hybride
hybrid_search_instance = None

def get_singleton_hybrid_search(embeddings_manager, chroma_manager, alpha=0.7):
    """
    R√©cup√®re ou initialise l'instance de recherche hybride
    
    Args:
        embeddings_manager: Instance du gestionnaire d'embeddings
        chroma_manager: Instance du gestionnaire ChromaDB
        alpha: Poids pour les scores vectoriels (1-alpha pour BM25)
    
    Returns:
        HybridSearch: Instance de recherche hybride
    """
    global hybrid_search_instance
    
    if hybrid_search_instance is None:
        logger.info("Initialisation de la recherche hybride (vectorielle + BM25)...")
        hybrid_search_instance = HybridSearch(chroma_manager, alpha=alpha)
        
        # R√©cup√©rer tous les documents de ChromaDB pour entra√Æner BM25
        logger.info("R√©cup√©ration des documents pour l'indexation BM25...")
        all_docs = chroma_manager.get_all_documents_with_content()
        
        if all_docs:
            logger.info(f"Indexation de {len(all_docs)} documents avec BM25...")
            hybrid_search_instance.fit(all_docs)
        else:
            logger.warning("Aucun document disponible pour l'indexation BM25")
    
    return hybrid_search_instance

def display_contract_search_results(query: str, n_results: int = 5, use_hybrid: bool = True) -> None:
    """
    Search in the contract database

    Args:
        query: Search query
        n_results: Number of results to return
        use_hybrid: Whether to use hybrid search (BM25 + semantic)
    """
    logger.info(f"\nüîç Recherche: {query}")

    # Initialize managers
    embeddings_manager = TextVectorizer()
    chroma_manager = VectorDBInterface(embeddings_manager)
    
    # Choose search method
    if use_hybrid:
        logger.info("Utilisation de la recherche hybride (vectorielle + BM25)...")
        hybrid_search = get_singleton_hybrid_search(embeddings_manager, chroma_manager)
        results = hybrid_search.search(query, n_results=n_results)
    else:
        # Standard vector search
        logger.info("Utilisation de la recherche vectorielle standard...")
        results = chroma_manager.search(query, n_results=n_results)

    # Display results
    logger.info(f"\nüìä R√©sultats ({len(results)} trouv√©s):")
    for i, result in enumerate(results, 1):
        logger.info(f"\n--- R√©sultat {i} ---")
        logger.info(
            f"Section: {result['metadata'].get('section_number', 'Non sp√©cifi√©')}"
        )
        logger.info(
            f"Hi√©rarchie: {result['metadata'].get('hierarchy', 'Non sp√©cifi√©')}"
        )
        logger.info(
            f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}"
        )
        logger.info(f"Contenu: {result['document'][:200]}...")
        logger.info(f"Distance vectorielle: {result['distance']:.4f}")
        
        # Afficher les scores BM25 si disponibles
        if 'bm25_score' in result:
            logger.info(f"Score BM25: {result['bm25_score']:.4f}")
            logger.info(f"Score combin√©: {result['combined_score']:.4f}")

def load_or_recreate_knowledge_graph(chroma_manager, embeddings_manager):
    """
    Load existing graph or build a new one if needed
    
    Args:
        chroma_manager: ChromaDB manager instance
        embeddings_manager: Embeddings manager instance
    
    Returns:
        KnowledgeGraph: The loaded or built knowledge graph
    """
    import os
    import pickle
    
    graph_path = "knowledge_graph.pkl"
    
    # Check if graph file exists
    if os.path.exists(graph_path):
        try:
            logger.info(f"üìÇ Chargement du graphe existant depuis {graph_path}...")
            with open(graph_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du graphe: {str(e)}")
    
    # If no graph exists or loading failed, build a new one
    logger.info("üîÑ Construction d'un nouveau graphe de connaissances...")
    
    # Get all documents from ChromaDB
    docs_dict = chroma_manager.get_all_documents()
    
    # Convert the dictionary to the expected format for build_graph
    all_docs = []
    
    # We need the actual content, so fetch the documents with their content
    all_docs_with_content = chroma_manager.collection.get()
    
    # Create a mapping of id -> content
    id_to_content = {}
    if "ids" in all_docs_with_content and "documents" in all_docs_with_content:
        for i, doc_id in enumerate(all_docs_with_content["ids"]):
            if i < len(all_docs_with_content["documents"]):
                id_to_content[doc_id] = all_docs_with_content["documents"][i]
    
    # Now create the list of dicts expected by build_graph
    for doc_id, metadata in docs_dict.items():
        content = id_to_content.get(doc_id, "")
        all_docs.append({
            "content": content,
            "metadata": metadata
        })
    
    logger.info(f"Pr√©paration de {len(all_docs)} documents pour la construction du graphe")
    
    # Build graph
    graph_manager = GraphManager(chroma_manager, embeddings_manager)
    graph = graph_manager.build_graph(all_docs)
    
    # Save graph for future use
    try:
        with open(graph_path, 'wb') as f:
            pickle.dump(graph, f)
        logger.info(f"‚úÖ Graphe sauvegard√© dans {graph_path}")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Impossible de sauvegarder le graphe: {str(e)}")
    
    return graph

def expand_results_with_graph(graph, initial_results, n_additional=2):
    """
    Expand search results by traversing the knowledge graph
    
    Args:
        graph: Knowledge graph
        initial_results: Initial results from vector search
        n_additional: Number of additional results to retrieve
        
    Returns:
        List of additional results from graph traversal
    """
    graph_results = []
    processed_ids = set()
    
    # Add initial result IDs to processed set to avoid duplicates
    for result in initial_results:
        if 'id' in result:
            processed_ids.add(result['id'])
    
    # Find nodes that correspond to initial results
    for result in initial_results:
        # Try to find matching node by content or metadata
        result_content = result.get('document', '')
        
        # Find node that most closely matches this result
        matching_node_id = None
        for node_id, node in graph.nodes.items():
            if result_content in node.content:
                matching_node_id = node_id
                break
        
        if not matching_node_id:
            continue
            
        # Find related nodes through different relation types
        for edge in graph.edges:
            if edge.source == matching_node_id:
                target_node_id = edge.target
                
                # Skip if already processed
                if target_node_id in processed_ids:
                    continue
                    
                processed_ids.add(target_node_id)
                
                # Get the target node and create a result
                target_node = graph.nodes[target_node_id]
                
                # Create a result object similar to the ChromaDB results
                graph_result = {
                    'document': target_node.content,
                    'metadata': target_node.metadata,
                    'source_type': 'graph',
                    'relation_type': edge.relation_type,
                    'distance': 1.0 - edge.weight,  # Convert weight to distance
                    'id': target_node_id
                }
                
                graph_results.append(graph_result)
                
                # Limit number of additional results
                if len(graph_results) >= n_additional:
                    return graph_results
    
    return graph_results

def combine_vector_and_graph_results(vector_results, graph_results):
    """
    Merge vector-based and graph-based results, avoiding duplicates
    
    Args:
        vector_results: Results from vector search
        graph_results: Results from graph traversal
        
    Returns:
        List of combined results
    """
    # Start with vector results
    combined_results = list(vector_results)
    
    # Track document contents to avoid duplicates
    seen_contents = set(result.get('document', '')[:100] for result in vector_results)
    
    # Add graph results that aren't duplicates
    for result in graph_results:
        content_preview = result.get('document', '')[:100]
        if content_preview not in seen_contents:
            seen_contents.add(content_preview)
            combined_results.append(result)
    
    return combined_results

def combine_similarity_and_date_hits(similarity_results, date_results, n_context):
    """
    Combine results from similarity search with date-specific results,
    keeping the TOP_K results from similarity search and adding date results as a complement.
    
    Args:
        similarity_results: Results from standard similarity search
        date_results: Results from date-specific search
        n_context: Maximum number of results to return
        
    Returns:
        List of combined results with dates as a complement
    """
    # Start with similarity results
    combined_results = list(similarity_results)
    
    # Track document IDs to avoid duplicates
    seen_ids = set(result.get('id', '') for result in similarity_results)
    
    # Add date results that aren't duplicates
    for result in date_results:
        result_id = result.get('id', '')
        if result_id and result_id not in seen_ids:
            seen_ids.add(result_id)
            # Mark as date-specific result
            result['from_date_search'] = True
            combined_results.append(result)
    
    # Sort results - prioritizing similarity results first, then date results by their score
    # This ensures that TOP_K similarity results are preserved at the top
    combined_results.sort(key=lambda x: (x.get('from_date_search', False), x.get('distance', 1.0)))
    
    # Limit to requested number of results
    return combined_results[:n_context]

def retrieve_contract_sources_with_alternatives(query: str, n_context: int = int(os.getenv("TOP_K", 5)), use_graph: bool = False,
                         similarity_threesold: float = float(os.getenv("SIMILARITY_THRESHOLD", 0.6)), 
                         use_alternatives: bool = True, model_name: str = 'mistral-small3.1') -> list:
    """
    R√©cup√®re les sources pertinentes en utilisant des requ√™tes alternatives pour am√©liorer la recherche

    Args:
        query: Question de l'utilisateur
        n_context: Nombre de chunks pertinents √† utiliser
        use_graph: Utiliser le graphe de connaissances
        similarity_threesold: Seuil de similarit√© pour le filtrage
        use_alternatives: Utiliser des requ√™tes alternatives
        model_name: Mod√®le √† utiliser pour g√©n√©rer les alternatives

    Returns:
        list: Liste des sources pertinentes avec d√©duplication
    """
    logger.info(f"Recherche de sources pour: {query}")
    
    # Initialize managers
    embeddings_manager = TextVectorizer()
    chroma_manager = VectorDBInterface(embeddings_manager)
    reranker_manager = Reranker("mxbai-rerank-large-v2")

    graph_manager = None
    knowledge_graph = None
    if use_graph:
        logger.info("Utilisation du graphe de connaissances pour enrichir le contexte...")
        graph_manager = GraphManager(chroma_manager, embeddings_manager)
        knowledge_graph = load_or_recreate_knowledge_graph(chroma_manager, embeddings_manager)

    # Structure pour suivre les doublons
    seen_ids = set()
    seen_content_hashes = set()
    all_results = []
    
    # Commencer par la requ√™te originale
    logger.info("Recherche avec la requ√™te originale...")
    original_results = chroma_manager.search(query, n_results=n_context)
    logger.info(f"Trouv√© {len(original_results)} r√©sultats pour la requ√™te originale")
    
    # Ajouter les r√©sultats originaux
    for result in original_results:
        result_id = result.get('id', '')
        content = result.get('document', '')
        content_hash = hash(content)
        
        if (result_id and result_id in seen_ids) or (content_hash in seen_content_hashes):
            continue
            
        if result_id:
            seen_ids.add(result_id)
        seen_content_hashes.add(content_hash)
        
        result["source_query"] = "Original"
        all_results.append(result)
    
    # G√©n√©rer et utiliser des requ√™tes alternatives si demand√©
    if use_alternatives:
        try:
            logger.info("G√©n√©ration de requ√™tes alternatives...")
            alternative_queries = generate_queries(query, model_name)
            logger.info(f"G√©n√©r√© {len(alternative_queries)} requ√™tes alternatives")
            
            # Calculer le nombre de r√©sultats par requ√™te alternative
            remaining_slots = max(0, n_context * 2 - len(all_results))  # Permettre jusqu'√† 2x plus de r√©sultats
            results_per_alt_query = max(1, remaining_slots // len(alternative_queries)) if alternative_queries else 0
            
            for i, alt_query in enumerate(alternative_queries, 1):
                logger.info(f"Recherche avec requ√™te alternative {i}: {alt_query}")
                alt_results = chroma_manager.search(alt_query, n_results=results_per_alt_query)
                
                unique_alt_results = 0
                for result in alt_results:
                    result_id = result.get('id', '')
                    content = result.get('document', '')
                    content_hash = hash(content)
                    
                    if (result_id and result_id in seen_ids) or (content_hash in seen_content_hashes):
                        continue
                        
                    if result_id:
                        seen_ids.add(result_id)
                    seen_content_hashes.add(content_hash)
                    
                    result["source_query"] = f"Alternative {i}: {alt_query[:50]}..."
                    all_results.append(result)
                    unique_alt_results += 1
                
                logger.info(f"Ajout√© {unique_alt_results} nouveaux r√©sultats uniques de la requ√™te alternative {i}")
                
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des requ√™tes alternatives: {e}")
            logger.warning("Poursuite avec la requ√™te originale uniquement")
    
    logger.info(f"Total de {len(all_results)} r√©sultats uniques collect√©s")
    
    # Check if this is a date-related query
    date_related_terms = [
        'date', '√©ch√©ance', 'd√©lai', 'terme', 'expire', 'expiration', 'calendrier', 
        'planning', 'horaire', 'jour', 'mois', 'ann√©e', 'trimestre', 'semestre',
        'p√©riode', 'dur√©e', 'temps', 'chronologie', 'deadline', 'livraison', 
        'anniversaire', 'signature', 'pr√©avis',
        'deadline', 'calendar', 'schedule', 'day', 'month', 'year', 'quarter', 
        'semester', 'period', 'duration', 'time', 'timeline', 'delivery', 
        'anniversary', 'signature', 'notice'
    ]
    is_date_query = any(term in query.lower() for term in date_related_terms)

    # Standard filtering based on similarity threshold
    filtered_results = [d for d in all_results if d['distance'] <= 1-similarity_threesold]
    logger.info(f"Apr√®s filtrage par seuil de similarit√©: {len(filtered_results)} r√©sultats restent")

    # For date-related queries, complement with date-specific results
    if is_date_query:
        logger.info("Requ√™te li√©e aux dates d√©tect√©e, s√©lection de chunks pertinents avec dates")
        date_results = chroma_manager.select_context(all_results, query)
        logger.info(f"S√©lection bas√©e sur les dates trouv√©e: {len(date_results)} r√©sultats avec informations de date")
        
        filtered_results = combine_similarity_and_date_hits(filtered_results, date_results, n_context * 2)
        logger.info(f"Apr√®s fusion des r√©sultats de similarit√© et de date: {len(filtered_results)} r√©sultats totaux")

    # If no results pass the threshold, use the original results
    if not filtered_results and all_results:
        logger.warning("Aucun r√©sultat n'a pass√© le seuil de similarit√©. Utilisation des r√©sultats non filtr√©s √† la place.")
        filtered_results = all_results
    
    # Only attempt reranking if we have results
    if filtered_results:
        try:
            # Limiter le nombre de r√©sultats pour le reranking
            rerank_limit = min(len(filtered_results), n_context * 3)  # Rerank jusqu'√† 3x le nombre demand√©
            reranked_docs = reranker_manager.rerank(query, filtered_results[:rerank_limit], n_context)
            logger.info(f"Reranking r√©ussi de {len(reranked_docs)} documents")
            filtered_results = reranked_docs
        except Exception as e:
            logger.error(f"Erreur lors du reranking: {e}")
            logger.warning("Utilisation des r√©sultats filtr√©s sans reranking")
            # Limiter aux n_context meilleurs r√©sultats par distance
            filtered_results = sorted(filtered_results, key=lambda x: x.get('distance', 1.0))[:n_context]
    else:
        logger.warning("Aucun document disponible pour le reranking")
    
    # Use graph results if available
    if use_graph and knowledge_graph and filtered_results:
        graph_results = expand_results_with_graph(knowledge_graph, filtered_results, n_additional=2)
        combined_results = combine_vector_and_graph_results(filtered_results, graph_results)
    else:
        combined_results = filtered_results
    
    # Add query info to each result
    for result in combined_results:
        result["query"] = query
    
    return combined_results

def retrieve_contract_sources(query: str, n_context: int = int(os.getenv("TOP_K", 5)), use_graph: bool = False,
                         similarity_threesold: float = float(os.getenv("SIMILARITY_THRESHOLD", 0.6)),
                         use_hybrid: bool = bool(os.getenv("USE_HYBRID", "True").lower() == "true")) -> list:
    """
    R√©cup√®re uniquement les sources pertinentes pour une requ√™te sans g√©n√©rer de r√©ponse LLM

    Args:
        query: Question de l'utilisateur
        n_context: Nombre de chunks pertinents √† utiliser
        use_graph: Utiliser le graphe de connaissances
        similarity_threesold: Seuil de similarit√© pour le filtrage
        use_hybrid: Utiliser la recherche hybride (BM25 + s√©mantique)

    Returns:
        list: Liste des sources pertinentes
    """
    logger.info(f"Recherche de sources pour: {query}")

    # Initialize managers
    embeddings_manager = TextVectorizer()
    chroma_manager = VectorDBInterface(embeddings_manager)
    reranker_manager = Reranker("mxbai-rerank-large-v2")

    graph_manager = None
    knowledge_graph = None
    if use_graph:
        logger.info("Utilisation du graphe de connaissances pour enrichir le contexte...")
        graph_manager = GraphManager(chroma_manager, embeddings_manager)
        # Load or build the graph
        knowledge_graph = load_or_recreate_knowledge_graph(chroma_manager, embeddings_manager)

    # Check if this is a date-related query
    date_related_terms = [
        'date', '√©ch√©ance', 'd√©lai', 'terme', 'expire', 'expiration', 'calendrier', 
        'planning', 'horaire', 'jour', 'mois', 'ann√©e', 'trimestre', 'semestre',
        'p√©riode', 'dur√©e', 'temps', 'chronologie', 'deadline', 'livraison', 
        'anniversaire', 'signature', 'pr√©avis',
        'deadline', 'calendar', 'schedule', 'day', 'month', 'year', 'quarter', 
        'semester', 'period', 'duration', 'time', 'timeline', 'delivery', 
        'anniversary', 'signature', 'notice'
    ]
    is_date_query = any(term in query.lower() for term in date_related_terms)

    # Get search results
    if use_hybrid:
        logger.info("Utilisation de la recherche hybride (vectorielle + BM25)...")
        hybrid_search = get_singleton_hybrid_search(embeddings_manager, chroma_manager)
        # Pour une recherche hybride, le filtrage par seuil est d√©j√† int√©gr√© dans la fonction search
        results = hybrid_search.search(query, n_results=n_context, min_semantic_score=similarity_threesold)
        logger.info(f"Found {len(results)} initial hybrid search results")
        
        # Pour les requ√™tes hybrides, on consid√®re les r√©sultats directement sans filtrage suppl√©mentaire
        filtered_results = results
    else:
        # Standard vector search
        logger.info("Utilisation de la recherche vectorielle standard...")
        results = chroma_manager.search(query, n_results=n_context)
        logger.info(f"Found {len(results)} initial search results")
    
        # Standard filtering based on similarity threshold for non-date queries
        filtered_results = [d for d in results if d['distance'] <= 1-similarity_threesold]
        logger.info(f"After filtering by similarity threshold: {len(filtered_results)} results remain")

    # For date-related queries, complement with date-specific results
    if is_date_query:
        logger.info("Date-related query detected, selecting relevant chunks with dates")
        # Get date-specific results
        date_results = chroma_manager.select_context(results, query)
        logger.info(f"Date-based selection found: {len(date_results)} results with date information")
        
        # Merge similarity results with date-specific results
        filtered_results = combine_similarity_and_date_hits(filtered_results, date_results, n_context * 2)
        logger.info(f"After merging similarity and date results: {len(filtered_results)} total results")

    # If no results pass the threshold, use the original results
    if not filtered_results and results:
        logger.warning("No results passed the similarity threshold. Using unfiltered results instead.")
        filtered_results = results
    
    # Only attempt reranking if we have results
    if filtered_results:
        try:
            reranked_docs = reranker_manager.rerank(query, filtered_results, n_context)
            logger.info(f"Successfully reranked {len(reranked_docs)} documents")
            filtered_results = reranked_docs
        except Exception as e:
            logger.error(f"Error during reranking: {e}")
            logger.warning("Using filtered results without reranking")
    else:
        logger.warning("No documents available for reranking")
    
    # Use graph results if available
    if use_graph and knowledge_graph:
        graph_results = expand_results_with_graph(knowledge_graph, results, n_additional=2)
        # Combine results (ensuring no duplicates)
        combined_results = combine_vector_and_graph_results(filtered_results, graph_results)
    else:
        combined_results = filtered_results
    
    # Add query info to each result
    for result in combined_results:
        result["query"] = query
    
    return combined_results

def chat_with_contract(query: str, n_context: int = int(os.getenv("TOP_K", 5)), use_graph: bool = False, 
temperature: float = float(os.getenv("TEMPERATURE", 0.5)), similarity_threesold: float = float(os.getenv("SIMILARITY_THRESHOLD", 0.6)), 
model: str = os.getenv("LLM_MODEL", "mistral-small3.1:latest"), context_window: int = int(os.getenv("CONTEXT_WINDOW", 0)), 
history: str = "", use_hybrid: bool = bool(os.getenv("USE_HYBRID", "True").lower() == "true")) -> tuple:
    """
    Chat with the contract using embeddings for context and Ollama for generation

    Args:
        query: User's question
        n_context: Number of relevant chunks to use as context
        use_graph: Whether to use graph-based context expansion
        use_hybrid: Whether to use hybrid search (BM25 + semantic)
        
    Returns:
        tuple: (response, sources) where response is the LLM's answer and sources is a list of sources used
    """
    logger.info(f"\nüí¨ Chat: {query}")

    # Initialize managers
    embeddings_manager = TextVectorizer()
    chroma_manager = VectorDBInterface(embeddings_manager)
    reranker_manager = Reranker("mxbai-rerank-large-v2")

    graph_manager = None
    knowledge_graph = None
    if use_graph:
        logger.info("üîç Utilisation du graphe de connaissances pour enrichir le contexte...")
        graph_manager = GraphManager(chroma_manager, embeddings_manager)
        # Load or build the graph
        knowledge_graph = load_or_recreate_knowledge_graph(chroma_manager, embeddings_manager)

    # Check if this is a date-related query
    date_related_terms = [
        'date', '√©ch√©ance', 'd√©lai', 'terme', 'expire', 'expiration', 'calendrier', 
        'planning', 'horaire', 'jour', 'mois', 'ann√©e', 'trimestre', 'semestre',
        'p√©riode', 'dur√©e', 'temps', 'chronologie', 'deadline', 'livraison', 
        'anniversaire', 'signature', 'pr√©avis',
        'deadline', 'calendar', 'schedule', 'day', 'month', 'year', 'quarter', 
        'semester', 'period', 'duration', 'time', 'timeline', 'delivery', 
        'anniversary', 'signature', 'notice'
    ]
    is_date_query = any(term in query.lower() for term in date_related_terms)

    # Get search results
    if use_hybrid:
        logger.info("Utilisation de la recherche hybride (vectorielle + BM25)...")
        hybrid_search = get_singleton_hybrid_search(embeddings_manager, chroma_manager)
        # Pour une recherche hybride, le filtrage par seuil est d√©j√† int√©gr√© dans la fonction search
        results = hybrid_search.search(query, n_results=n_context, min_semantic_score=similarity_threesold)
        logger.info(f"Found {len(results)} initial hybrid search results")
        
        # Pour les requ√™tes hybrides, on consid√®re les r√©sultats directement sans filtrage suppl√©mentaire
        filtered_results = results
    else:
        # Standard vector search
        logger.info("Utilisation de la recherche vectorielle standard...")
        results = chroma_manager.search(query, n_results=n_context)
        logger.info(f"Found {len(results)} initial search results")
    
        # Standard filtering based on similarity threshold for non-date queries
        filtered_results = [d for d in results if d['distance'] <= 1-similarity_threesold]
        logger.info(f"After filtering by similarity threshold: {len(filtered_results)} results remain")

    # For date-related queries, complement with date-specific results
    if is_date_query:
        logger.info("Date-related query detected, selecting relevant chunks with dates")
        # Get date-specific results
        date_results = chroma_manager.select_context(results, query)
        logger.info(f"Date-based selection found: {len(date_results)} results with date information")
        
        # Merge similarity results with date-specific results
        filtered_results = combine_similarity_and_date_hits(filtered_results, date_results, n_context * 2)
        logger.info(f"After merging similarity and date results: {len(filtered_results)} total results")

    # If no results pass the threshold, use the original results
    if not filtered_results and results:
        logger.warning("No results passed the similarity threshold. Using unfiltered results instead.")
        filtered_results = results
    
    # Only attempt reranking if we have results
    if filtered_results:
        try:
            reranked_docs = reranker_manager.rerank(query, filtered_results, n_context)
            logger.info(f"Successfully reranked {len(reranked_docs)} documents")
        except Exception as e:
            logger.error(f"Error during reranking: {e}")
            logger.warning("Using filtered results without reranking")
            reranked_docs = filtered_results
    else:
        logger.warning("No documents available for reranking")
        reranked_docs = []
    
    # Use graph results if available
    if use_graph and knowledge_graph:
        graph_results = expand_results_with_graph(knowledge_graph, results, n_additional=2)
        # Combine results (ensuring no duplicates)
        combined_results = combine_vector_and_graph_results(filtered_results, graph_results)
    else:
        combined_results = filtered_results
    
    # Check if we have any results to use
    if not combined_results:
        no_results_response = "Je n'ai pas trouv√© d'information pertinente dans la base de connaissances pour r√©pondre √† votre question. Pourriez-vous reformuler ou pr√©ciser votre demande?"
        print("\nü§ñ R√©ponse :")
        print(no_results_response)
        logger.warning("No relevant documents found for the query")
        return no_results_response, []
    
    # Prepare context for the prompt
    context_parts = []
    for result in combined_results:
        # En-t√™te avec les m√©tadonn√©es
        header = (
            f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}\n"
            f"Section: {result['metadata'].get('section_number', 'Non sp√©cifi√©')}\n"
            f"Hi√©rarchie: {result['metadata'].get('hierarchy', 'Non sp√©cifi√©')}"
        )

        # Ajouter les dates si pr√©sentes dans les m√©tadonn√©es
        if result['metadata'].get('dates'):
            header += f"\nDates: {result['metadata'].get('dates')}"

        # Contenu adapt√© selon le type (r√©sum√© ou original)
        if result.get("is_summary", False):
            content = (
                f"\nR√©sum√©:\n{result['document']}\n"
                f"Contenu d√©taill√© si n√©cessaire:\n{result.get('original_content', 'Non disponible')}"
            )
        else:
            content = f"\nContenu:\n{result['document']}"

        # Ajouter la source au contexte
        context_parts.append(f"{header}\n{content}")

    # Joindre toutes les parties du contexte
    context = "\n\n---\n\n".join(context_parts)

    # Create the prompt with context
    prompt = f"""
    You are an assistant specializing in contract analysis.

    Here is the history of the conversation:
    {history}

    Here is the relevant context extracted from the documents. ‚Ä¶
    {context}

    User's question: {query}

    Always double-check any summary by consulting the detailed content.
    If the context lacks the information, state it explicitly.
    If a history is present and non-null, extract useful elements from it to answer the initial question.
    """


    # Get response from Ollama
    response = llm_chat_call_with_ollama(prompt, temperature, model, context_window)
    logger.info("\nü§ñ R√©ponse :")
    logger.info(response)
    print("\nü§ñ R√©ponse :")
    print(response)
    print("\nüìö Sources :")
    print("=" * 80)

    # Display sources with metadata
    logger.info("\nüìö Sources :")
    logger.info("=" * 80)
    for i, result in enumerate(combined_results, 1):
        logger.info("\n" + "-" * 40)
        logger.info(f"\nSource {i}/{len(combined_results)}")
        
        # Afficher le type de source (r√©sum√© ou original)
        if result.get("source_type") == "graph":
            logger.info("üìä Source obtenue via le graphe de connaissances")
            logger.info(f"Relation: {result.get('relation_type', 'Non sp√©cifi√©')}")
        logger.info("-" * 40)
        logger.info(f"Hierarchie: {result['metadata'].get('hierarchy', 'Non sp√©cifi√©')}")
        logger.info(f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}")

        # Afficher les dates si pr√©sentes
        if result['metadata'].get('dates'):
            logger.info(f"Dates: {result['metadata'].get('dates')}")
            print(f"Dates: {result['metadata'].get('dates')}")

        logger.info(f"Distance: {result['distance']:.4f}")
        
        # Afficher les scores BM25 si disponibles
        if 'bm25_score' in result:
            logger.info(f"Score BM25: {result['bm25_score']:.4f}")
            logger.info(f"Score combin√©: {result['combined_score']:.4f}")
            print(f"Score BM25: {result['bm25_score']:.4f}")
            print(f"Score combin√©: {result['combined_score']:.4f}")

        # Afficher le contenu
        if result.get("is_summary", False):
            logger.info("\nR√©sum√© utilis√©:")
            logger.info(result["document"])
            logger.info("\nContenu original:")
            logger.info(result.get("original_content", "Non disponible") + "...")
        else:
            logger.info("\nContenu:")
            logger.info(result["document"] + "...")

        logger.info("-" * 40)

    # Afficher les statistiques
    summaries = sum(1 for r in combined_results if r.get("is_summary", False))
    graph_sources = sum(1 for r in combined_results if r.get("source_type") == "graph")
    date_sources = sum(1 for r in combined_results if r.get('metadata', {}).get('dates'))
    hybrid_sources = sum(1 for r in combined_results if r.get('hybrid_search', False))
    
    logger.info(f"\nüìä Statistiques des sources:")
    logger.info(f"- Total: {len(combined_results)}")
    logger.info(f"- R√©sum√©s: {summaries}")
    logger.info(f"- Contenus originaux: {len(combined_results) - summaries - graph_sources}")
    logger.info(f"- Sources du graphe: {graph_sources}")
    logger.info(f"- Sources avec dates: {date_sources}")
    logger.info(f"- Sources via recherche hybride: {hybrid_sources}")
    
    return response

def chat_with_contract_using_query_decomposition(query: str, n_context: int = int(os.getenv("TOP_K", 5)), use_graph: bool = False, 
temperature: float = float(os.getenv("TEMPERATURE", 0.5)), similarity_threesold: float = float(os.getenv("SIMILARITY_THRESHOLD", 0.6)), 
model: str = os.getenv("LLM_MODEL", "mistral-small3.1:latest"), context_window: int = int(os.getenv("CONTEXT_WINDOW", 0)),
max_total_sources: int = 15) -> tuple:
    """
    Chat with the contract by decomposing the query into sub-queries, getting answers for each,
    and then synthesizing a final response with all sources.

    Args:
        query: User's question
        n_context: Number of relevant chunks to use as context
        use_graph: Whether to use graph-based context expansion
        temperature: Temperature for LLM generation
        similarity_threesold: Threshold for similarity filtering
        model: LLM model to use
        context_window: Context window size for the LLM
        max_total_sources: Maximum total number of sources to collect across all sub-queries
    """
    logger.info(f"\nüí¨ Requ√™te complexe: {query}")
    
    # Structure pour suivre les doublons
    seen_ids = set()
    seen_content_hashes = set()
    all_sources = []
    
    # Commencer par r√©cup√©rer les sources pour la question principale
    logger.info("Recherche de sources pour la question principale...")
    
    # Allocation plus √©quilibr√©e: environ 1/3 pour la question principale au lieu de 1/2
    # avec un minimum de 2 sources et un maximum de n_context/3
    main_question_sources = max(2, min(n_context // 3, 3))
    
    main_sources = retrieve_contract_sources(query, main_question_sources, use_graph, similarity_threesold)
    logger.info(f"Trouv√© {len(main_sources)} sources pour la question principale")
    
    # Filtrer et ajouter les sources principales
    main_unique_sources = []
    for source in main_sources:
        source_id = source.get('id', '')
        content = source.get('document', '')
        content_hash = hash(content)
        
        # V√©rifier si cette source est un doublon
        if (source_id and source_id in seen_ids) or (content_hash in seen_content_hashes):
            continue
            
        # Marquer comme vue
        if source_id:
            seen_ids.add(source_id)
        seen_content_hashes.add(content_hash)
        
        # Ajouter la question principale √† la source
        source["sub_query"] = "Question principale"
        main_unique_sources.append(source)
        all_sources.append(source)
    
    logger.info(f"Apr√®s d√©duplication: {len(main_unique_sources)}/{len(main_sources)} sources uniques pour la question principale")
    
    # Ensuite, d√©composer la requ√™te en sous-questions
    sub_queries = define_subqueries(query)
    logger.info(f"Requ√™te d√©compos√©e en {len(sub_queries.questions)} sous-questions")
    
    # Ajuster dynamiquement le nombre de sources par sous-requ√™te
    # R√©partir les sources restantes entre les sous-questions
    remaining_sources = max_total_sources - len(all_sources)
    num_subqueries = len(sub_queries.questions)
    
    if num_subqueries > 0 and remaining_sources > 0:
        # Garantir au moins 2 sources par sous-question si possible
        sources_per_subquery = max(2, remaining_sources // num_subqueries)
        
        # Si le nombre calcul√© d√©passe n_context, on le plafonne
        # Cela permet d'√©viter de demander trop de sources pour une seule sous-question
        sources_per_subquery = min(sources_per_subquery, n_context // 2)
        
        logger.info(f"Sources par sous-requ√™te: {sources_per_subquery} (total restant: {remaining_sources})")
    else:
        sources_per_subquery = 0
        logger.info("Aucune source suppl√©mentaire ne sera collect√©e pour les sous-requ√™tes")
    
    sub_questions_with_sources = []
    
    # Ajouter la question principale avec ses sources
    sub_questions_with_sources.append({
        "question": query,
        "sources": main_unique_sources,
        "is_main_question": True
    })
    
    # Traiter les sous-questions seulement si on a de la place pour plus de sources
    if sources_per_subquery > 0:
        for i, sub_query in enumerate(sub_queries.questions, 1):
            logger.info(f"\nüîç Sous-question {i}/{num_subqueries}: {sub_query}")
            
            # R√©cup√©rer uniquement les sources sans g√©n√©rer de r√©ponse
            sources = retrieve_contract_sources(sub_query, sources_per_subquery, use_graph, similarity_threesold)
            logger.info(f"Trouv√© {len(sources)} sources pour la sous-question")
            
            # Filtrer les sources pour √©viter les doublons
            unique_sources_for_query = []
            for source in sources:
                source_id = source.get('id', '')
                content = source.get('document', '')
                content_hash = hash(content)
                
                # V√©rifier si cette source est un doublon
                if (source_id and source_id in seen_ids) or (content_hash in seen_content_hashes):
                    logger.debug(f"Source dupliqu√©e ignor√©e: {source_id}")
                    continue
                    
                # Marquer comme vue
                if source_id:
                    seen_ids.add(source_id)
                seen_content_hashes.add(content_hash)
                
                # Ajouter la sous-question √† la source
                source["sub_query"] = sub_query
                unique_sources_for_query.append(source)
                all_sources.append(source)
            
            logger.info(f"Apr√®s d√©duplication: {len(unique_sources_for_query)}/{len(sources)} sources uniques pour cette sous-question")
            
            # Stocker la sous-question avec ses sources uniques
            sub_questions_with_sources.append({
                "question": sub_query, 
                "sources": unique_sources_for_query,
                "is_main_question": False
            })
            
            # Si on atteint le nombre maximum de sources, arr√™ter
            if len(all_sources) >= max_total_sources:
                logger.info(f"Nombre maximum de sources atteint ({max_total_sources})")
                break
    
    logger.info(f"Total de {len(all_sources)} sources uniques collect√©es pour la question principale et les sous-questions")
    
    # Si aucune source n'a √©t√© trouv√©e
    if not all_sources:
        no_results_response = "Je n'ai pas trouv√© d'information pertinente dans la base de connaissances pour r√©pondre √† votre question ou ses sous-questions. Pourriez-vous reformuler ou pr√©ciser votre demande?"
        print("\nü§ñ R√©ponse :")
        print(no_results_response)
        logger.warning("Aucune source pertinente trouv√©e pour les sous-questions")
        return no_results_response, []
    
    # Pr√©parer le contexte pour la synth√®se finale
    context_parts = []
    for source in all_sources:
        # En-t√™te avec les m√©tadonn√©es
        header = (
            f"Document: {source['metadata'].get('document_title', 'Non sp√©cifi√©')}\n"
            f"Section: {source['metadata'].get('section_number', 'Non sp√©cifi√©')}\n"
            f"Hi√©rarchie: {source['metadata'].get('hierarchy', 'Non sp√©cifi√©')}\n"
            f"En r√©ponse √†: {source.get('sub_query', 'Question principale')}"
        )

        # Ajouter les dates si pr√©sentes dans les m√©tadonn√©es
        if source['metadata'].get('dates'):
            header += f"\nDates: {source['metadata'].get('dates')}"

        # Contenu adapt√© selon le type
        if source.get("is_summary", False):
            content = (
                f"\nR√©sum√©:\n{source['document']}\n"
                f"Contenu d√©taill√© si n√©cessaire:\n{source.get('original_content', 'Non disponible')}"
            )
        else:
            content = f"\nContenu:\n{source['document']}"

        # Ajouter la source au contexte
        context_parts.append(f"{header}\n{content}")

    # Joindre toutes les parties du contexte
    context = "\n\n---\n\n".join(context_parts)
    
    # Estimer la taille du contexte en tokens
    # Une estimation courante est d'environ 4 caract√®res par token en moyenne pour les langues europ√©ennes
    context_token_estimate = len(context) / 4
    
    # R√©serve pour le prompt (instructions, question, etc.) - environ 1000 tokens
    prompt_overhead = 1000
    available_context_tokens = context_window - prompt_overhead
    
    # V√©rifier si le contexte est trop grand et l'ajuster si n√©cessaire
    if context_token_estimate > available_context_tokens:
        logger.warning(f"Contexte trop grand: ~{int(context_token_estimate)} tokens estim√©s > {available_context_tokens} disponibles")
        
        # Strat√©gie d'ajustement: r√©duire proportionnellement le contenu de chaque source
        reduction_ratio = available_context_tokens / context_token_estimate
        logger.info(f"R√©duction du contexte √† {int(reduction_ratio * 100)}% de sa taille originale")
        
        # Reconstruire le contexte avec contenu r√©duit
        adjusted_context_parts = []
        for source in all_sources:
            # Conserver l'en-t√™te complet
            header = (
                f"Document: {source['metadata'].get('document_title', 'Non sp√©cifi√©')}\n"
                f"Section: {source['metadata'].get('section_number', 'Non sp√©cifi√©')}\n"
                f"Hi√©rarchie: {source['metadata'].get('hierarchy', 'Non sp√©cifi√©')}\n"
                f"En r√©ponse √†: {source.get('sub_query', 'Question principale')}"
            )
            
            if source['metadata'].get('dates'):
                header += f"\nDates: {source['metadata'].get('dates')}"
                
            # R√©duire proportionnellement le contenu
            content = source.get('document', '')
            max_content_length = int(len(content) * reduction_ratio)
            if len(content) > max_content_length:
                content = content[:max_content_length] + " [...]"
                
            adjusted_part = f"{header}\n\nContenu:\n{content}"
            adjusted_context_parts.append(adjusted_part)
            
        # Reconstruire le contexte
        context = "\n\n---\n\n".join(adjusted_context_parts)
        new_token_estimate = len(context) / 4
        logger.info(f"Taille de contexte ajust√©e: ~{int(new_token_estimate)} tokens estim√©s")
    
    # Extraire la liste des sous-questions (sans la question principale)
    sub_questions_list = [q["question"] for q in sub_questions_with_sources if not q.get("is_main_question", False)]
    
    # Cr√©er le prompt pour la synth√®se finale
    final_prompt = f"""Tu es un assistant sp√©cialis√© dans l'analyse de contrats.

J'ai d√©compos√© la question de l'utilisateur en plusieurs sous-questions:
{", ".join(sub_questions_list)}

Voici les extraits pertinents de documents pour r√©pondre √† la question principale et aux sous-questions:

{context}

Question principale de l'utilisateur: {query}

En utilisant uniquement les informations fournies dans ces extraits de documents, synth√©tise une r√©ponse compl√®te et coh√©rente 
√† la question principale. Organise ta r√©ponse de mani√®re logique et structur√©e. Si certaines sous-questions ne peuvent pas √™tre 
r√©pondues avec les informations disponibles, indique-le clairement.
"""
    
    # Obtenir la synth√®se finale
    final_response = llm_chat_call_with_ollama(final_prompt, temperature, model, context_window)
    
    # Afficher la r√©ponse finale
    logger.info("\nü§ñ R√©ponse synth√©tis√©e:")
    logger.info(final_response)
    print("\nü§ñ R√©ponse synth√©tis√©e:")
    print(final_response)
    
    # Afficher toutes les sources utilis√©es
    print(f"\nüìö Sources utilis√©es ({len(all_sources)} au total):")
    print("=" * 80)
    
    # Afficher les sources regroup√©es par sous-question
    sources_by_query = {}
    for source in all_sources:
        sub_query = source.get("sub_query", "Question principale")
        if sub_query not in sources_by_query:
            sources_by_query[sub_query] = []
        sources_by_query[sub_query].append(source)
    
    # Afficher d'abord les sources pour la question principale
    if "Question principale" in sources_by_query:
        main_sources = sources_by_query["Question principale"]
        print(f"\n--- Sources pour la question principale ({len(main_sources)} sources) ---")
        
        for i, source in enumerate(main_sources, 1):
            print(f"\nSource {i}/{len(main_sources)}")
            
            # Afficher les informations de la source
            print(f"Document: {source['metadata'].get('document_title', 'Non sp√©cifi√©')}")
            print(f"Hi√©rarchie: {source['metadata'].get('hierarchy', 'Non sp√©cifi√©')}")
            
            # Afficher les dates si pr√©sentes
            if source['metadata'].get('dates'):
                print(f"Dates: {source['metadata'].get('dates')}")
                
            # Afficher un extrait du contenu
            content_preview = source['document'][:200] + "..." if len(source['document']) > 200 else source['document']
            print(f"Extrait: {content_preview}")
            print("-" * 40)
    
    # Puis les sources pour chaque sous-question
    for sub_query, sources in sources_by_query.items():
        if sub_query == "Question principale":
            continue  # D√©j√† affich√©
            
        print(f"\n--- Sources pour: '{sub_query}' ({len(sources)} sources) ---")
        
        for i, source in enumerate(sources, 1):
            print(f"\nSource {i}/{len(sources)}")
            
            # Afficher les informations de la source
            print(f"Document: {source['metadata'].get('document_title', 'Non sp√©cifi√©')}")
            print(f"Hi√©rarchie: {source['metadata'].get('hierarchy', 'Non sp√©cifi√©')}")
            
            # Afficher les dates si pr√©sentes
            if source['metadata'].get('dates'):
                print(f"Dates: {source['metadata'].get('dates')}")
                
            # Afficher un extrait du contenu
            content_preview = source['document'][:200] + "..." if len(source['document']) > 200 else source['document']
            print(f"Extrait: {content_preview}")
            print("-" * 40)
    
    return final_response, all_sources

def chat_with_contract_using_query_alternatives(query: str, n_context: int = int(os.getenv("TOP_K", 5)), use_graph: bool = False, 
temperature: float = float(os.getenv("TEMPERATURE", 0.5)), similarity_threesold: float = float(os.getenv("SIMILARITY_THRESHOLD", 0.6)), 
model: str = os.getenv("LLM_MODEL", "mistral-small3.1:latest"), context_window: int = int(os.getenv("CONTEXT_WINDOW", 0)),
use_alternatives: bool = True) -> tuple:
    """
    Chat with the contract using alternative queries to improve retrieval

    Args:
        query: User's question
        n_context: Number of relevant chunks to use as context
        use_graph: Whether to use graph-based context expansion
        temperature: Temperature for LLM generation
        similarity_threesold: Threshold for similarity filtering
        model: LLM model to use
        context_window: Context window size for the LLM
        use_alternatives: Whether to use alternative queries for better retrieval
        
    Returns:
        tuple: (response, sources) where response is the LLM's answer and sources is a list of sources used
    """
    logger.info(f"\nüí¨ Chat avec requ√™tes alternatives: {query}")

    # Get sources using alternative queries
    combined_results = retrieve_contract_sources_with_alternatives(
        query, n_context, use_graph, similarity_threesold, use_alternatives, model.split(':')[0]
    )
    
    # Check if we have any results to use
    if not combined_results:
        no_results_response = "Je n'ai pas trouv√© d'information pertinente dans la base de connaissances pour r√©pondre √† votre question. Pourriez-vous reformuler ou pr√©ciser votre demande?"
        print("\nü§ñ R√©ponse :")
        print(no_results_response)
        logger.warning("No relevant documents found for the query")
        return no_results_response, []
    
    # Prepare context for the prompt
    context_parts = []
    for result in combined_results:
        # En-t√™te avec les m√©tadonn√©es
        header = (
            f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}\n"
            f"Section: {result['metadata'].get('section_number', 'Non sp√©cifi√©')}\n"
            f"Hi√©rarchie: {result['metadata'].get('hierarchy', 'Non sp√©cifi√©')}\n"
            f"Source de recherche: {result.get('source_query', 'Original')}"
        )

        # Ajouter les dates si pr√©sentes dans les m√©tadonn√©es
        if result['metadata'].get('dates'):
            header += f"\nDates: {result['metadata'].get('dates')}"

        # Contenu adapt√© selon le type (r√©sum√© ou original)
        if result.get("is_summary", False):
            content = (
                f"\nR√©sum√©:\n{result['document']}\n"
                f"Contenu d√©taill√© si n√©cessaire:\n{result.get('original_content', 'Non disponible')}"
            )
        else:
            content = f"\nContenu:\n{result['document']}"

        # Ajouter la source au contexte
        context_parts.append(f"{header}\n{content}")

    # Joindre toutes les parties du contexte
    context = "\n\n---\n\n".join(context_parts)

    # Create the prompt with context
    prompt = f"""You are an assistant specializing in contract analysis.
    Here is the relevant context extracted from the documents using multiple search strategies (original query + alternative formulations).
    For each section, you either have a summary with the detailed content available or the original content itself.
    First use the summaries to get an overview, then consult the detailed content if you need more precision.

    {context}

    User's question: {query}

    Provide a precise answer based on the context given.
    If you use a summary, check the detailed content to ensure your answer's accuracy.
    If you can't find the information in the context, state that clearly."""

    # Get response from Ollama
    response = llm_chat_call_with_ollama(prompt, temperature, model, context_window)
    logger.info("\nü§ñ R√©ponse :")
    logger.info(response)
    print("\nü§ñ R√©ponse :")
    print(response)
    print("\nüìö Sources (avec requ√™tes alternatives) :")
    print("=" * 80)

    # Display sources with metadata
    logger.info("\nüìö Sources (avec requ√™tes alternatives) :")
    logger.info("=" * 80)
    
    # Group sources by search query
    sources_by_query = {}
    for result in combined_results:
        source_query = result.get('source_query', 'Original')
        if source_query not in sources_by_query:
            sources_by_query[source_query] = []
        sources_by_query[source_query].append(result)
    
    # Display sources grouped by query type
    for query_type, sources in sources_by_query.items():
        logger.info(f"\n--- Sources de: {query_type} ({len(sources)} sources) ---")
        print(f"\n--- Sources de: {query_type} ({len(sources)} sources) ---")
        
        for i, result in enumerate(sources, 1):
            logger.info("\n" + "-" * 40)
            logger.info(f"\nSource {i}/{len(sources)}")
            
            # Afficher le type de source (r√©sum√© ou original)
            if result.get("source_type") == "graph":
                logger.info("üìä Source obtenue via le graphe de connaissances")
                logger.info(f"Relation: {result.get('relation_type', 'Non sp√©cifi√©')}")
                print("üìä Source obtenue via le graphe de connaissances")
                print(f"Relation: {result.get('relation_type', 'Non sp√©cifi√©')}")
            
            logger.info("-" * 40)
            logger.info(f"Hierarchie: {result['metadata'].get('hierarchy', 'Non sp√©cifi√©')}")
            logger.info(f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}")
            print(f"Hierarchie: {result['metadata'].get('hierarchy', 'Non sp√©cifi√©')}")
            print(f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}")

            # Afficher les dates si pr√©sentes
            if result['metadata'].get('dates'):
                logger.info(f"Dates: {result['metadata'].get('dates')}")
                print(f"Dates: {result['metadata'].get('dates')}")

            logger.info(f"Distance: {result['distance']:.4f}")
            print(f"Distance: {result['distance']:.4f}")

            # Afficher le contenu
            if result.get("is_summary", False):
                logger.info("\nR√©sum√© utilis√©:")
                logger.info(result["document"][:200] + "...")
                print(f"Contenu: {result['document'][:200]}...")
            else:
                logger.info("\nContenu:")
                logger.info(result["document"][:200] + "...")
                print(f"Contenu: {result['document'][:200]}...")

            logger.info("-" * 40)

    # Afficher les statistiques
    original_sources = len(sources_by_query.get('Original', []))
    alternative_sources = sum(len(sources) for query_type, sources in sources_by_query.items() if query_type != 'Original')
    graph_sources = sum(1 for r in combined_results if r.get("source_type") == "graph")
    date_sources = sum(1 for r in combined_results if r.get('metadata', {}).get('dates'))
    hybrid_sources = sum(1 for r in combined_results if r.get('hybrid_search', False))
    
    logger.info(f"\nüìä Statistiques des sources:")
    logger.info(f"- Total: {len(combined_results)}")
    logger.info(f"- Sources requ√™te originale: {original_sources}")
    logger.info(f"- Sources requ√™tes alternatives: {alternative_sources}")
    logger.info(f"- Sources du graphe: {graph_sources}")
    logger.info(f"- Sources avec dates: {date_sources}")
    logger.info(f"- Sources via recherche hybride: {hybrid_sources}")
    
    print(f"\nüìä Statistiques des sources:")
    print(f"- Total: {len(combined_results)}")
    print(f"- Sources requ√™te originale: {original_sources}")
    print(f"- Sources requ√™tes alternatives: {alternative_sources}")
    print(f"- Sources du graphe: {graph_sources}")
    print(f"- Sources avec dates: {date_sources}")
    print(f"- Sources via recherche hybride: {hybrid_sources}")
    
    return response, combined_results

def query_classifier(query: str, n_context: int = int(os.getenv("TOP_K", 5)), use_graph: bool = False, 
                 temperature: float = float(os.getenv("TEMPERATURE", 0.5)), 
                 similarity_threesold: float = float(os.getenv("SIMILARITY_THRESHOLD", 0.6)), 
                 model: str = os.getenv("LLM_MODEL", "mistral-small3.1:latest"), 
                 context_window: int = int(os.getenv("CONTEXT_WINDOW", 0)),
                 use_classification: bool = False,
                 use_hybrid: bool = bool(os.getenv("USE_HYBRID", "True").lower() == "true")) -> tuple:
    """
    Process a user query by first classifying it as RAG or LLM and then routing to the appropriate handler.
    
    Args:
        query: User's question
        n_context: Number of relevant chunks to use as context
        use_graph: Whether to use graph-based context expansion
        temperature: Temperature for LLM generation
        similarity_threesold: Threshold for similarity filtering
        model: LLM model to use
        context_window: Context window size for the LLM
        use_classification: Whether to use query classification (if False, always use RAG)
        use_hybrid: Whether to use hybrid search (BM25 + semantic)
        
    Returns:
        tuple: (response, sources) where response is the LLM's answer and sources is a list of sources used
    """

    similar_conversations = retrieve_similar_conversations(query, os.getenv("HISTORY_DB_FILE"))

    history_summary = ""

    if similar_conversations:
            history_summary = "\n".join(
                [
                    f"User: {conv['question']}\nAssistant: {conv['response']}"
                    for conv in similar_conversations
                ]
            )

            if "#force" not in query:

                prompt = f"""
                    You are an assistant specializing in contract analysis.

                    Here is the history of the conversation:
                    {history_summary}

                    User's question: {query}

                    Always double-check any summary by consulting the detailed content.
                    If the context lacks the information, state it explicitly.
                    If a history is present and non-null, extract useful elements from it to answer the initial question.
                    """
                

                response = llm_chat_call_with_ollama(prompt, temperature, model, context_window)
                add_conversation_in_history_db(
                    os.getenv("HISTORY_DB_FILE"), query, response
                )
                print("\nü§ñ R√©ponse :")
                print(response)
                return response

    if "#force" in query:
        query = query.replace("#force", "")

    if history_summary == "":
        full_history = get_history(os.getenv("HISTORY_DB_FILE"))
        # on prend les 8 derni√®res entr√©es (4 paires)
        recent = full_history[-8:]
        # regrouper en paires User/Assistant
        pairs = []
        for i in range(0, len(recent), 2):
            if i + 1 < len(recent):
                user_msg = recent[i]["content"]
                assistant_msg = recent[i + 1]["content"]
                pairs.append((user_msg, assistant_msg))
        # on garde au plus les 4 derni√®res paires
        pairs = pairs[-4:]
        # transformer en r√©sum√© textuel
        history_summary = "\n".join(
            [f"User: {q}\nAssistant: {a}" for q, a in pairs]
        )
        logger.debug(
            f"üîç R√©sum√© fallback de l'historique (4 derniers √©changes) :\n{history_summary}"
        )
    else:
        # on garde au plus 4 messages similaires trouv√©s
        similar_messages = similar_messages[:4]
        history_summary = "\n".join(
            [
                f"User: {conv['question']}\nAssistant: {conv['response']}"
                for conv in similar_messages
            ]
        )
        logger.debug(
            f"üîç R√©sum√© historique (similar messages) :\n{history_summary}"
        )

    if use_classification:
        # Classify the query as either "RAG" or "LLM"
        classification = determine_inference_mode(query, verbose=True)
        logger.info(f"Query classification result: {classification}")
        
        if classification == "llm":
            # For LLM queries, use direct generation without context
            logger.info(f"Using direct LLM processing for query: {query}")
            # Create a simple prompt for the LLM
            prompt = f"""You are a contract analysis assistant. 
            Answer the following question about contracts to the best of your ability, based on general knowledge.
            If you're uncertain, mention that you don't have specific contract details.
            
            Question: {query}
            """
            response = llm_chat_call_with_ollama(prompt=prompt, temperature=temperature, model=model, context_window=context_window)
            logger.info("\nü§ñ R√©ponse :")
            logger.info(response)
            print("\nü§ñ R√©ponse :")
            print(response)
            add_conversation_in_history_db(
                os.getenv("HISTORY_DB_FILE"), query, response
            )
            return response  # No sources for LLM-only processing
        
    # If classification is "RAG" or classification is disabled, use RAG processing
    logger.info(f"Using RAG processing for query: {query}")
    response = chat_with_contract(
        query, n_context, use_graph, temperature, 
        similarity_threesold, model, context_window, 
        history_summary, use_hybrid
    )
    add_conversation_in_history_db(
            os.getenv("HISTORY_DB_FILE"), query, response
    )
