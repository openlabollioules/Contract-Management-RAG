import uuid
from typing import Dict, List, Optional
from pathlib import Path
import os
import re

import chromadb
from chromadb.config import Settings

from utils.logger import setup_logger
from core.llm_service import LLMService

from .text_vectorizer import TextVectorizer

# Configurer le logger pour ce module
logger = setup_logger(__file__)


class VectorDBInterface:
    def __init__(
        self,
        embeddings_manager: TextVectorizer,
        persist_directory: str = "chroma_db",
        collection_name: str = "contracts",
        llm_service = None
    ):
        """
        Initialize ChromaDB manager with an embeddings manager

        Args:
            embeddings_manager: Instance of TextVectorizer for generating embeddings
            persist_directory: Directory to persist ChromaDB data
            collection_name: Name of the collection to use
            llm_service: LLM service for advanced text analysis
        """
        self.embeddings_manager = embeddings_manager
        self.persist_directory = persist_directory
        self.llm_service = llm_service
        logger.info(
            f"Initialisation de VectorDBInterface (persist_directory={persist_directory}, collection={collection_name})"
        )

        # Ensure the persist directory exists with proper permissions
        persist_path = Path(persist_directory)
        if not persist_path.exists():
            persist_path.mkdir(mode=0o777, parents=True, exist_ok=True)
        
        # Set permissions for all existing files and directories
        for item in persist_path.rglob('*'):
            try:
                item.chmod(0o777)
            except Exception as e:
                logger.warning(f"Could not set permissions for {item}: {e}")
        
        # Set umask to ensure new files are created with proper permissions
        old_umask = os.umask(0)
        try:
            # Initialize ChromaDB client with settings
            logger.debug(f"Cr√©ation du client ChromaDB persistant")
            self.client = chromadb.PersistentClient(
                path=persist_directory,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True,
                    is_persistent=True
                ),
            )

            # Create or get collection
            logger.debug(f"R√©cup√©ration ou cr√©ation de la collection '{collection_name}'")
            self.collection = self.client.get_or_create_collection(
                name=collection_name, metadata={"hnsw:space": "cosine"}
            )
            logger.info(f"Collection '{collection_name}' initialis√©e")
        finally:
            # Restore original umask
            os.umask(old_umask)

    def document_exists(self, filename: str) -> bool:
        """
        V√©rifie si un document avec le nom de fichier donn√© existe d√©j√† dans la base de donn√©es

        Args:
            filename: Nom du fichier √† v√©rifier

        Returns:
            bool: True si le document existe, False sinon
        """
        logger.info(f"V√©rification de l'existence du document: {filename}")

        try:
            # R√©cup√©rer tous les documents et leurs m√©tadonn√©es pour une v√©rification compl√®te
            all_docs = self.collection.get()

            if not all_docs or "metadatas" not in all_docs or not all_docs["metadatas"]:
                logger.debug("Aucun document trouv√© dans la base de donn√©es")
                return False

            # Convertir le nom du fichier en minuscules pour une comparaison insensible √† la casse
            filename_lower = filename.lower()

            # Rechercher le nom de fichier dans les m√©tadonn√©es de document_title ou filename
            doc_exists = False
            for i, metadata in enumerate(all_docs["metadatas"]):
                # V√©rifier dans document_title
                if "document_title" in metadata:
                    doc_title = (
                        metadata["document_title"].lower()
                        if metadata["document_title"]
                        else ""
                    )
                    logger.debug(
                        f"Document en base: '{metadata['document_title']}', √† comparer avec: '{filename}'"
                    )

                    # V√©rifier si le nom du fichier est dans le titre du document
                    if filename_lower in doc_title:
                        logger.info(
                            f"Document trouv√© dans la base de donn√©es: '{metadata['document_title']}' contient '{filename}'"
                        )
                        doc_exists = True
                        break

                # V√©rifier dans filename (si disponible)
                if "filename" in metadata and metadata["filename"]:
                    db_filename = metadata["filename"].lower()
                    logger.debug(
                        f"V√©rification avec filename: '{db_filename}' vs '{filename_lower}'"
                    )

                    if filename_lower == db_filename or filename_lower in db_filename:
                        logger.info(
                            f"Document trouv√© dans la base de donn√©es par filename: '{db_filename}'"
                        )
                        doc_exists = True
                        break

            if not doc_exists:
                # Essayer une m√©thode alternative de recherche par contenu
                logger.debug(
                    "Tentative de recherche alternative dans le contenu des documents"
                )
                try:
                    # Utiliser la recherche s√©mantique pour trouver des documents avec le nom du fichier
                    results = self.collection.query(query_texts=[filename], n_results=5)

                    if len(results["ids"]) > 0 and len(results["ids"][0]) > 0:
                        # V√©rifier si un de ces documents contient vraiment le nom du fichier
                        for i, doc in enumerate(results["documents"][0]):
                            if filename_lower in doc.lower():
                                logger.info(
                                    f"Document trouv√© par recherche s√©mantique qui contient '{filename}'"
                                )
                                doc_exists = True
                                break
                except Exception as e:
                    logger.warning(f"Erreur lors de la recherche alternative: {e}")

            logger.info(
                f"R√©sultat final de la v√©rification - Document '{filename}' existe: {doc_exists}"
            )
            return doc_exists

        except Exception as e:
            logger.warning(
                f"Erreur lors de la v√©rification de l'existence du document: {e}"
            )
            # Par pr√©caution, retourner False en cas d'erreur
            return False

    def delete_document(self, filename: str) -> bool:
        """
        Supprime tous les chunks associ√©s √† un document sp√©cifique

        Args:
            filename: Nom du fichier du document √† supprimer

        Returns:
            bool: True si la suppression a r√©ussi, False sinon
        """
        logger.warning(f"Suppression du document: {filename}")

        try:
            # R√©cup√©rer tous les documents et leurs m√©tadonn√©es
            all_docs = self.collection.get()

            if not all_docs or "metadatas" not in all_docs or not all_docs["metadatas"]:
                logger.warning(f"Aucun document trouv√© pour la suppression")
                return False

            # Convertir le nom du fichier en minuscules pour une comparaison insensible √† la casse
            filename_lower = filename.lower()

            # Collecter les IDs des chunks √† supprimer
            chunk_ids_to_delete = []

            for i, metadata in enumerate(all_docs["metadatas"]):
                # V√©rifier dans document_title
                if "document_title" in metadata and metadata["document_title"]:
                    doc_title = metadata["document_title"].lower()
                    if filename_lower in doc_title:
                        chunk_ids_to_delete.append(all_docs["ids"][i])
                        logger.debug(
                            f"ID √† supprimer (par document_title): {all_docs['ids'][i]} pour '{metadata['document_title']}'"
                        )
                        continue

                # V√©rifier dans filename (si disponible)
                if "filename" in metadata and metadata["filename"]:
                    db_filename = metadata["filename"].lower()
                    if filename_lower == db_filename or filename_lower in db_filename:
                        chunk_ids_to_delete.append(all_docs["ids"][i])
                        logger.debug(
                            f"ID √† supprimer (par filename): {all_docs['ids'][i]} pour '{metadata['filename']}'"
                        )
                        continue

                # Chercher dans le contenu du document
                if i < len(all_docs["documents"]) and all_docs["documents"][i]:
                    doc_content = all_docs["documents"][i].lower()
                    if filename_lower in doc_content:
                        # V√©rifier que c'est bien une mention significative du fichier
                        if (
                            f"document: " in doc_content
                            and filename_lower
                            in doc_content.split("document: ")[1].split("\n")[0].lower()
                        ):
                            chunk_ids_to_delete.append(all_docs["ids"][i])
                            logger.debug(
                                f"ID √† supprimer (par contenu): {all_docs['ids'][i]}"
                            )

            if not chunk_ids_to_delete:
                logger.warning(f"Aucun document trouv√© avec le nom '{filename}'")
                return False

            # Supprimer tous les chunks trouv√©s
            logger.info(
                f"Suppression de {len(chunk_ids_to_delete)} chunks associ√©s au document '{filename}'"
            )
            self.collection.delete(ids=chunk_ids_to_delete)

            logger.info(f"Document '{filename}' supprim√© avec succ√®s")
            return True
        except Exception as e:
            logger.error(f"Erreur lors de la suppression du document: {e}")
            return False

    def _extract_keywords(self, text: str) -> list:
        """
        Extract keywords from text using LLM analysis.
        Falls back to basic keyword detection if LLM is not available.

        Args:
            text: Text to analyze

        Returns:
            List of detected keywords
        """
        # If LLM service is not available, use fallback immediately
        if not self.llm_service or not self.llm_service.client:
            logger.debug("LLM service not available, using fallback keyword detection")
            return self._extract_keywords_fallback(text)

        try:
            # Prepare the prompt for the LLM
            prompt = f"""Analyze the following text and identify important keywords related to contracts, business terms, and legal concepts.
            Focus on terms that would be relevant for contract management and legal analysis.
            Return only the keywords, separated by commas.

            Text to analyze:
            {text}

            Keywords:"""

            # Get response from LLM
            response = self.llm_service.generate(prompt)
            
            # Process the response to get keywords
            if response:
                # Split by comma and clean up each keyword
                keywords = [kw.strip() for kw in response.split(',') if kw.strip()]
                logger.debug(f"Keywords detected by LLM: {keywords}")
                return keywords
            else:
                logger.warning("No keywords detected by LLM, using fallback")
                return self._extract_keywords_fallback(text)

        except Exception as e:
            logger.error(f"Error in LLM keyword extraction: {str(e)}")
            # Fallback to basic keyword detection if LLM fails
            return self._extract_keywords_fallback(text)

    def _extract_keywords_fallback(self, text: str) -> list:
        """
        Fallback method for keyword extraction using predefined terms.
        Used when LLM analysis fails.

        Args:
            text: Text to analyze

        Returns:
            List of detected keywords without duplicates
        """
        # Dictionary to store terms and their translations
        terms_dict = {
            # Termes contractuels de base
            "montant": "amount",
            "√©ch√©ance": "due date",
            "livraison": "delivery",
            "p√©nalit√©": "penalty",
            "date": "date",
            "contrat": "contract",
            "signature": "signature",
            "avenant": "amendment",
            "r√©siliation": "termination",
            "paiement": "payment",
            "garantie": "warranty",
            "obligation": "obligation",
            "clause": "clause",
            
            # Nouveaux termes contractuels
            "parties": "parties",
            "objet du contrat": "scope of the contract",
            "livraisons": "deliverables",
            "frais": "fees",
            "confidentialit√©": "confidentiality",
            "propri√©t√© intellectuelle": "intellectual property",
            "clause de r√©siliation": "termination clause",
            "conformit√© l√©gale": "legal compliance",
            "support technique": "technical support",
            "responsabilit√©s": "responsibilities",
            "d√©lais": "milestones",
            "r√©solution des litiges": "dispute resolution",
            "force majeure": "force majeure",
            "acc√®s aux donn√©es": "data access",
            "s√©curit√© des donn√©es": "data security",
            "licence": "license",
            "exclusivit√©": "exclusivity",
            "dur√©e du contrat": "contract term",
            "r√©siliation pour cause": "termination for cause"
        }

        # Set to store unique found terms
        found_terms = set()

        # Search for terms in text
        for term_fr, term_en in terms_dict.items():
            if re.search(rf"\b{term_fr}\b", text, re.IGNORECASE):
                found_terms.add(term_fr)
            if re.search(rf"\b{term_en}\b", text, re.IGNORECASE):
                found_terms.add(term_en)

        return list(found_terms)

    def add_documents(
        self,
        chunks: List[Dict],
        metadata: Optional[List[Dict]] = None,
        ids: Optional[List[str]] = None,
    ) -> None:
        """
        Add documents to ChromaDB with their embeddings

        Args:
            chunks: List of document chunks (text content)
            metadata: Optional list of metadata dictionaries
            ids: Optional list of document IDs
        """
        if not chunks:
            logger.warning(
                "Tentative d'ajout de documents avec une liste de chunks vide"
            )
            return

        logger.info(f"Ajout de {len(chunks)} documents √† ChromaDB")

        # D√©tecter les dates dans chaque chunk avant l'ajout √† la base de donn√©es
        for chunk in chunks:
            # D√©tecter les dates dans le contenu du chunk
            dates = self._detect_dates(chunk.get('content', ''))
            if dates:
                # Ajouter les dates aux m√©tadonn√©es du chunk
                if 'metadata' not in chunk:
                    chunk['metadata'] = {}
                # Convertir la liste de dates en string pour ChromaDB
                chunk['metadata']['dates'] = '; '.join(dates)
            keywords = self._extract_keywords(chunk.get('content', ''))
            if keywords:
                if 'metadata' not in chunk:
                    chunk['metadata'] = {}
                chunk['metadata']['keywords'] = ', '.join(keywords)

        # Afficher les chunks avec les dates d√©tect√©es
        print_chunks_with_dates(chunks)

        # Generate embeddings
        logger.debug("G√©n√©ration des embeddings pour les chunks")
        texts = [chunk["content"] for chunk in chunks]
        embeddings = self.embeddings_manager.get_embeddings(texts)
        logger.debug(f"Embeddings g√©n√©r√©s: {len(embeddings)}")

        # Use metadata from chunks if not provided externally
        if metadata is None:
            logger.debug("Utilisation des m√©tadonn√©es contenues dans les chunks")
            metadata = [
                chunk.get("metadata", {"source": "document"}) for chunk in chunks
            ]

        # Ensure each metadata dict has at least one attribute
        logger.debug("V√©rification des m√©tadonn√©es")
        for i, meta in enumerate(metadata):
            if not meta:
                metadata[i] = {"source": "document"}

        # Generate IDs if not provided
        if ids is None:
            logger.debug("G√©n√©ration d'IDs UUID pour les documents")
            ids = [str(uuid.uuid4()) for _ in chunks]

        # Add to ChromaDB
        logger.debug("Ajout des documents √† ChromaDB")
        self.collection.add(
            embeddings=embeddings, documents=texts, metadatas=metadata, ids=ids
        )
        logger.info(
            f"Documents ajout√©s avec succ√®s (collection: {self.collection.name})"
        )

    def _detect_dates(self, text: str) -> List[str]:
        """
        Detect dates in text using regex patterns.
        Supports various date formats commonly found in contracts.

        Args:
            text: Text to analyze

        Returns:
            List of detected dates
        """
        # Common date patterns in contracts
        date_patterns = [
            # DD/MM/YYYY or DD-MM-YYYY
            r'\b(0?[1-9]|[12][0-9]|3[01])[/-](0?[1-9]|1[0-2])[/-](19|20)\d{2}\b',
            # YYYY/MM/DD or YYYY-MM-DD
            r'\b(19|20)\d{2}[/-](0?[1-9]|1[0-2])[/-](0?[1-9]|[12][0-9]|3[01])\b',
            # Month DD, YYYY (e.g., "January 1, 2024")
            r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+(?:0?[1-9]|[12][0-9]|3[01])(?:st|nd|rd|th)?,?\s+(?:19|20)\d{2}\b',
            # DD Month YYYY (e.g., "1 January 2024")
            r'\b(?:0?[1-9]|[12][0-9]|3[01])(?:st|nd|rd|th)?\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+(?:19|20)\d{2}\b',
            # French date format (e.g., "le 1er janvier 2024")
            r'\ble\s+(?:0?[1-9]|[12][0-9]|3[01])(?:er|√®me)?\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+(?:19|20)\d{2}\b',
            # DD.MM.YYYY (e.g., "06.12.2007")
            r'\b(0?[1-9]|[12][0-9]|3[01])\.(0?[1-9]|1[0-2])\.(19|20)\d{2}\b',
            # Month DDth YYYY (e.g., "September 30th 2012")
            r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+(?:0?[1-9]|[12][0-9]|3[01])(?:st|nd|rd|th)\s+(?:19|20)\d{2}\b',
            # DDth Month YYYY (e.g., "22nd August 2017")
            r'\b(?:0?[1-9]|[12][0-9]|3[01])(?:st|nd|rd|th)\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+(?:19|20)\d{2}\b',
            # Month DDth, YYYY (e.g., "February 2nd, 2012")
            r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+(?:0?[1-9]|[12][0-9]|3[01])(?:st|nd|rd|th),\s+(?:19|20)\d{2}\b'
        ]

        dates = []
        for pattern in date_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                logger.debug(f"Date trouv√©e: {match.group(0)}")
                dates.append(match.group(0))

        return dates

    def search(
        self, query: str, n_results: int = 5, filter_metadata: Optional[Dict] = None
    ) -> List[Dict]:
        """
        Search for similar documents. Handles both summarized and non-summarized chunks intelligently.

        Args:
            query: Search query
            n_results: Number of results to return
            filter_metadata: Optional metadata filters

        Returns:
            List of results with documents and metadata
        """
        logger.info(
            f"Recherche dans ChromaDB: '{query}' (n_results={n_results}, filtres={filter_metadata})"
        )

        # Generate query embedding
        logger.debug("G√©n√©ration de l'embedding pour la requ√™te")
        query_embedding = self.embeddings_manager.get_embeddings([query])[0]

        # Search in ChromaDB
        logger.debug(f"Requ√™te dans la collection '{self.collection.name}'")
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results * 2,  # On r√©cup√®re plus de r√©sultats pour le post-traitement
            where=filter_metadata,
        )

        # Format and process results
        logger.debug(
            f"Formatage des r√©sultats ({len(results['ids'][0])} documents trouv√©s)"
        )
        formatted_results = []
        seen_originals = set()  # Pour suivre les contenus originaux d√©j√† vus

        for i in range(len(results["ids"][0])):
            metadata = results["metadatas"][0][i]
            is_summary = metadata.get("is_summary", "false").lower() == "true"
            original_content = metadata.get("original_content", "")

            # Si c'est un r√©sum√©, on ajoute le contenu original aux m√©tadonn√©es pour r√©f√©rence
            if is_summary:
                result = {
                    "id": results["ids"][0][i],
                    "document": results["documents"][0][i],
                    "metadata": metadata,
                    "distance": results["distances"][0][i],
                    "is_summary": True,
                    "original_content": original_content
                }
                formatted_results.append(result)
                seen_originals.add(original_content)

            # Si c'est un contenu original et qu'on n'a pas d√©j√† son r√©sum√©
            elif results["documents"][0][i] not in seen_originals:
                result = {
                    "id": results["ids"][0][i],
                    "document": results["documents"][0][i],
                    "metadata": metadata,
                    "distance": results["distances"][0][i],
                    "is_summary": False
                }
                formatted_results.append(result)

        # Trier par score de similarit√© et limiter au nombre demand√©
        formatted_results.sort(key=lambda x: x["distance"])
        formatted_results = formatted_results[:n_results]

        logger.info(f"Recherche termin√©e, {len(formatted_results)} r√©sultats")
        # Log des statistiques sur les r√©sum√©s vs originaux
        summaries = sum(1 for r in formatted_results if r.get("is_summary", False))
        logger.info(f"R√©partition : {summaries} r√©sum√©s, {len(formatted_results) - summaries} originaux")
        
        return formatted_results

    def delete_collection(self) -> None:
        """Delete the current collection"""
        logger.warning(f"Suppression de la collection '{self.collection.name}'")
        self.client.delete_collection(self.collection.name)
        logger.info(f"Collection '{self.collection.name}' supprim√©e")

    def reset(self) -> None:
        """Reset the database and recreate the collection"""
        logger.warning("R√©initialisation compl√®te de la base de donn√©es ChromaDB")
        try:
            # Delete the collection if it exists
            try:
                self.client.delete_collection(self.collection.name)
            except Exception as e:
                logger.debug(f"Collection deletion failed (might not exist): {e}")
            
            # Recreate the collection
            self.collection = self.client.create_collection(
                name=self.collection.name,
                metadata={"hnsw:space": "cosine"}
            )
            logger.info("Base de donn√©es r√©initialis√©e et collection recr√©√©e")
        except Exception as e:
            logger.error(f"Error during reset: {e}")
            # If something went wrong, try to ensure we have a valid collection
            self.collection = self.client.get_or_create_collection(
                name=self.collection.name,
                metadata={"hnsw:space": "cosine"}
            )

    def get_all_documents(self) -> Dict:
        """
        R√©cup√®re tous les documents de la base de donn√©es avec leurs m√©tadonn√©es

        Returns:
            Dict: Dictionnaire avec les IDs des documents comme cl√©s et leurs m√©tadonn√©es comme valeurs
        """
        logger.debug("R√©cup√©ration de tous les documents")

        try:
            # R√©cup√©rer tous les documents
            all_docs = self.collection.get()

            if not all_docs or "metadatas" not in all_docs or not all_docs["metadatas"]:
                logger.debug("Aucun document trouv√© dans la base de donn√©es")
                return {}

            # Cr√©er un dictionnaire avec les ID comme cl√©s et les m√©tadonn√©es comme valeurs
            result = {}
            for i, doc_id in enumerate(all_docs["ids"]):
                result[doc_id] = all_docs["metadatas"][i]

            logger.debug(f"R√©cup√©ration termin√©e - {len(result)} documents trouv√©s")
            return result

        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des documents: {e}")
            return {}

    def select_context(self, results: List[Dict], query: str) -> List[Dict]:
        # Prioritize chunks with matching metadata
        relevant_chunks = []
        for result in results:
            if (result['metadata'].get('keywords') and 
                any(kw in query.lower() for kw in result['metadata']['keywords'].split(','))):
                relevant_chunks.append(result)
        
        return relevant_chunks or results  # Fallback to all results if no metadata matches

def print_chunks_with_dates(chunks):
    print("\nüìù Affichage des chunks avec m√©tadonn√©es:")
    print("=" * 80)
    
    # Compteur total de mots-cl√©s
    total_keywords = set()
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\nChunk {i}/{len(chunks)}")
        print("-" * 40)
        
        # R√©cup√©rer toutes les m√©tadonn√©es
        metadata = chunk.get('metadata', {})
        
        # Afficher les m√©tadonn√©es de base
        print(f"üìÑ Document: {metadata.get('document_title', 'Sans titre')}")
        print(f"üìë Section: {metadata.get('section_number', 'Non sp√©cifi√©e')}")
        
        # Afficher les dates si pr√©sentes
        if metadata.get('dates'):
            print(f"\nüìÖ Dates d√©tect√©es: {metadata['dates']}")
        
        # Afficher les mots-cl√©s si pr√©sents
        if metadata.get('keywords'):
            keywords = metadata['keywords'].split(', ')
            total_keywords.update(keywords)
            print(f"\nüîë Mots-cl√©s trouv√©s dans ce chunk:")
            for kw in keywords:
                print(f"  ‚Ä¢ {kw}")
        
        # Afficher toutes les autres m√©tadonn√©es
        other_metadata = {k: v for k, v in metadata.items() 
                         if k not in ['document_title', 'section_number', 'dates', 'keywords']}
        if other_metadata:
            print("\nüìã Autres m√©tadonn√©es:")
            for key, value in other_metadata.items():
                print(f"  ‚Ä¢ {key}: {value}")
        
        print("\nüìù Contenu:")
        print(chunk.get('content', ''))
        print("-" * 40)
    
    # Afficher le r√©sum√© des mots-cl√©s √† la fin
    if total_keywords:
        print("\nüìä R√©sum√© des mots-cl√©s trouv√©s:")
        print("=" * 40)
        for kw in sorted(total_keywords):
            print(f"  ‚Ä¢ {kw}")
        print(f"\nTotal des mots-cl√©s uniques: {len(total_keywords)}")
