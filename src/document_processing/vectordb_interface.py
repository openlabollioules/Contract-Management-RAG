import uuid
from typing import Dict, List, Optional
from pathlib import Path
import os
import re

import chromadb
from chromadb.config import Settings

from utils.logger import setup_logger

from .text_vectorizer import TextVectorizer

# Configurer le logger pour ce module
logger = setup_logger(__file__)


class VectorDBInterface:
    def __init__(
        self,
        embeddings_manager: TextVectorizer,
        persist_directory: str = "chroma_db",
        collection_name: str = "contracts",
    ):
        """
        Initialize ChromaDB manager with an embeddings manager

        Args:
            embeddings_manager: Instance of TextVectorizer for generating embeddings
            persist_directory: Directory to persist ChromaDB data
            collection_name: Name of the collection to use
        """
        self.embeddings_manager = embeddings_manager
        self.persist_directory = persist_directory
        logger.info(
            f"Initialisation de VectorDBInterface (persist_directory={persist_directory}, collection={collection_name})"
        )

        # Ensure the persist directory exists with proper permissions
        persist_path = Path(persist_directory)
        if not persist_path.exists():
            persist_path.mkdir(mode=0o777, parents=True, exist_ok=True)
        
        # Set permissions for all existing files and directories
        for item in persist_path.rglob('*'):
            try:
                item.chmod(0o777)
            except Exception as e:
                logger.warning(f"Could not set permissions for {item}: {e}")
        
        # Set umask to ensure new files are created with proper permissions
        old_umask = os.umask(0)
        try:
            # Initialize ChromaDB client with settings
            logger.debug(f"Cr√©ation du client ChromaDB persistant")
            self.client = chromadb.PersistentClient(
                path=persist_directory,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True,
                    is_persistent=True
                ),
            )

            # Create or get collection
            logger.debug(f"R√©cup√©ration ou cr√©ation de la collection '{collection_name}'")
            self.collection = self.client.get_or_create_collection(
                name=collection_name, metadata={"hnsw:space": "cosine"}
            )
            logger.info(f"Collection '{collection_name}' initialis√©e")
        finally:
            # Restore original umask
            os.umask(old_umask)

    def document_exists(self, filename: str) -> bool:
        """
        V√©rifie si un document avec le nom de fichier donn√© existe d√©j√† dans la base de donn√©es

        Args:
            filename: Nom du fichier √† v√©rifier

        Returns:
            bool: True si le document existe, False sinon
        """
        logger.info(f"V√©rification de l'existence du document: {filename}")

        try:
            # R√©cup√©rer tous les documents et leurs m√©tadonn√©es pour une v√©rification compl√®te
            all_docs = self.collection.get()

            if not all_docs or "metadatas" not in all_docs or not all_docs["metadatas"]:
                logger.debug("Aucun document trouv√© dans la base de donn√©es")
                return False

            # Convertir le nom du fichier en minuscules pour une comparaison insensible √† la casse
            filename_lower = filename.lower()

            # Rechercher le nom de fichier dans les m√©tadonn√©es de document_title ou filename
            doc_exists = False
            for i, metadata in enumerate(all_docs["metadatas"]):
                # V√©rifier dans document_title
                if "document_title" in metadata:
                    doc_title = (
                        metadata["document_title"].lower()
                        if metadata["document_title"]
                        else ""
                    )
                    logger.debug(
                        f"Document en base: '{metadata['document_title']}', √† comparer avec: '{filename}'"
                    )

                    # V√©rifier si le nom du fichier est dans le titre du document
                    if filename_lower in doc_title:
                        logger.info(
                            f"Document trouv√© dans la base de donn√©es: '{metadata['document_title']}' contient '{filename}'"
                        )
                        doc_exists = True
                        break

                # V√©rifier dans filename (si disponible)
                if "filename" in metadata and metadata["filename"]:
                    db_filename = metadata["filename"].lower()
                    logger.debug(
                        f"V√©rification avec filename: '{db_filename}' vs '{filename_lower}'"
                    )

                    if filename_lower == db_filename or filename_lower in db_filename:
                        logger.info(
                            f"Document trouv√© dans la base de donn√©es par filename: '{db_filename}'"
                        )
                        doc_exists = True
                        break

            if not doc_exists:
                # Essayer une m√©thode alternative de recherche par contenu
                logger.debug(
                    "Tentative de recherche alternative dans le contenu des documents"
                )
                try:
                    # Utiliser la recherche s√©mantique pour trouver des documents avec le nom du fichier
                    results = self.collection.query(query_texts=[filename], n_results=5)

                    if len(results["ids"]) > 0 and len(results["ids"][0]) > 0:
                        # V√©rifier si un de ces documents contient vraiment le nom du fichier
                        for i, doc in enumerate(results["documents"][0]):
                            if filename_lower in doc.lower():
                                logger.info(
                                    f"Document trouv√© par recherche s√©mantique qui contient '{filename}'"
                                )
                                doc_exists = True
                                break
                except Exception as e:
                    logger.warning(f"Erreur lors de la recherche alternative: {e}")

            logger.info(
                f"R√©sultat final de la v√©rification - Document '{filename}' existe: {doc_exists}"
            )
            return doc_exists

        except Exception as e:
            logger.warning(
                f"Erreur lors de la v√©rification de l'existence du document: {e}"
            )
            # Par pr√©caution, retourner False en cas d'erreur
            return False

    def delete_document(self, filename: str) -> bool:
        """
        Supprime tous les chunks associ√©s √† un document sp√©cifique

        Args:
            filename: Nom du fichier du document √† supprimer

        Returns:
            bool: True si la suppression a r√©ussi, False sinon
        """
        logger.warning(f"Suppression du document: {filename}")

        try:
            # R√©cup√©rer tous les documents et leurs m√©tadonn√©es
            all_docs = self.collection.get()

            if not all_docs or "metadatas" not in all_docs or not all_docs["metadatas"]:
                logger.warning(f"Aucun document trouv√© pour la suppression")
                return False

            # Convertir le nom du fichier en minuscules pour une comparaison insensible √† la casse
            filename_lower = filename.lower()

            # Collecter les IDs des chunks √† supprimer
            chunk_ids_to_delete = []

            for i, metadata in enumerate(all_docs["metadatas"]):
                # V√©rifier dans document_title
                if "document_title" in metadata and metadata["document_title"]:
                    doc_title = metadata["document_title"].lower()
                    if filename_lower in doc_title:
                        chunk_ids_to_delete.append(all_docs["ids"][i])
                        logger.debug(
                            f"ID √† supprimer (par document_title): {all_docs['ids'][i]} pour '{metadata['document_title']}'"
                        )
                        continue

                # V√©rifier dans filename (si disponible)
                if "filename" in metadata and metadata["filename"]:
                    db_filename = metadata["filename"].lower()
                    if filename_lower == db_filename or filename_lower in db_filename:
                        chunk_ids_to_delete.append(all_docs["ids"][i])
                        logger.debug(
                            f"ID √† supprimer (par filename): {all_docs['ids'][i]} pour '{metadata['filename']}'"
                        )
                        continue

                # Chercher dans le contenu du document
                if i < len(all_docs["documents"]) and all_docs["documents"][i]:
                    doc_content = all_docs["documents"][i].lower()
                    if filename_lower in doc_content:
                        # V√©rifier que c'est bien une mention significative du fichier
                        if (
                            f"document: " in doc_content
                            and filename_lower
                            in doc_content.split("document: ")[1].split("\n")[0].lower()
                        ):
                            chunk_ids_to_delete.append(all_docs["ids"][i])
                            logger.debug(
                                f"ID √† supprimer (par contenu): {all_docs['ids'][i]}"
                            )

            if not chunk_ids_to_delete:
                logger.warning(f"Aucun document trouv√© avec le nom '{filename}'")
                return False

            # Supprimer tous les chunks trouv√©s
            logger.info(
                f"Suppression de {len(chunk_ids_to_delete)} chunks associ√©s au document '{filename}'"
            )
            self.collection.delete(ids=chunk_ids_to_delete)

            logger.info(f"Document '{filename}' supprim√© avec succ√®s")
            return True
        except Exception as e:
            logger.error(f"Erreur lors de la suppression du document: {e}")
            return False

    def add_documents(
        self,
        chunks: List[Dict],
        metadata: Optional[List[Dict]] = None,
        ids: Optional[List[str]] = None,
    ) -> None:
        """
        Add documents to ChromaDB with their embeddings

        Args:
            chunks: List of document chunks (text content)
            metadata: Optional list of metadata dictionaries
            ids: Optional list of document IDs
        """
        if not chunks:
            logger.warning(
                "Tentative d'ajout de documents avec une liste de chunks vide"
            )
            return

        logger.info(f"Ajout de {len(chunks)} documents √† ChromaDB")

        # D√©tecter les dates dans chaque chunk avant l'ajout √† la base de donn√©es
        for chunk in chunks:
            # D√©tecter les dates dans le contenu du chunk
            dates = self._detect_dates(chunk.get('content', ''))
            if dates:
                # Ajouter les dates aux m√©tadonn√©es du chunk
                if 'metadata' not in chunk:
                    chunk['metadata'] = {}
                # Convertir la liste de dates en string pour ChromaDB
                chunk['metadata']['dates'] = '; '.join(dates)

        # Afficher les chunks avec les dates d√©tect√©es
        print_chunks_with_dates(chunks)

        # Generate embeddings
        logger.debug("G√©n√©ration des embeddings pour les chunks")
        texts = [chunk["content"] for chunk in chunks]
        embeddings = self.embeddings_manager.get_embeddings(texts)
        logger.debug(f"Embeddings g√©n√©r√©s: {len(embeddings)}")

        # Use metadata from chunks if not provided externally
        if metadata is None:
            logger.debug("Utilisation des m√©tadonn√©es contenues dans les chunks")
            metadata = [
                chunk.get("metadata", {"source": "document"}) for chunk in chunks
            ]

        # Ensure each metadata dict has at least one attribute
        logger.debug("V√©rification des m√©tadonn√©es")
        for i, meta in enumerate(metadata):
            if not meta:
                metadata[i] = {"source": "document"}

        # Generate IDs if not provided
        if ids is None:
            logger.debug("G√©n√©ration d'IDs UUID pour les documents")
            ids = [str(uuid.uuid4()) for _ in chunks]

        # Add to ChromaDB
        logger.debug("Ajout des documents √† ChromaDB")
        self.collection.add(
            embeddings=embeddings, documents=texts, metadatas=metadata, ids=ids
        )
        logger.info(
            f"Documents ajout√©s avec succ√®s (collection: {self.collection.name})"
        )

    def _detect_dates(self, text: str) -> List[str]:
        """
        Detect dates in text using regex patterns.
        Supports various date formats commonly found in contracts.

        Args:
            text: Text to analyze

        Returns:
            List of detected dates
        """
        # Common date patterns in contracts
        date_patterns = [
            # DD/MM/YYYY or DD-MM-YYYY
            r'\b(0?[1-9]|[12][0-9]|3[01])[/-](0?[1-9]|1[0-2])[/-](19|20)\d{2}\b',
            # YYYY/MM/DD or YYYY-MM-DD
            r'\b(19|20)\d{2}[/-](0?[1-9]|1[0-2])[/-](0?[1-9]|[12][0-9]|3[01])\b',
            # Month DD, YYYY (e.g., "January 1, 2024")
            r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+(?:0?[1-9]|[12][0-9]|3[01]),\s+(?:19|20)\d{2}\b',
            # DD Month YYYY (e.g., "1 January 2024")
            r'\b(?:0?[1-9]|[12][0-9]|3[01])\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+(?:19|20)\d{2}\b',
            # French date format (e.g., "le 1er janvier 2024")
            r'\ble\s+(?:0?[1-9]|[12][0-9]|3[01])(?:er|√®me)?\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+(?:19|20)\d{2}\b',
        ]

        dates = []
        for pattern in date_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                logger.debug(f"Date trouv√©e: {match.group(0)}")
                dates.append(match.group(0))

        return dates

    def search(
        self, query: str, n_results: int = 5, filter_metadata: Optional[Dict] = None
    ) -> List[Dict]:
        """
        Search for similar documents. Handles both summarized and non-summarized chunks intelligently.

        Args:
            query: Search query
            n_results: Number of results to return
            filter_metadata: Optional metadata filters

        Returns:
            List of results with documents and metadata
        """
        logger.info(
            f"Recherche dans ChromaDB: '{query}' (n_results={n_results}, filtres={filter_metadata})"
        )

        # Generate query embedding
        logger.debug("G√©n√©ration de l'embedding pour la requ√™te")
        query_embedding = self.embeddings_manager.get_embeddings([query])[0]

        # Search in ChromaDB
        logger.debug(f"Requ√™te dans la collection '{self.collection.name}'")
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results * 2,  # On r√©cup√®re plus de r√©sultats pour le post-traitement
            where=filter_metadata,
        )

        # Format and process results
        logger.debug(
            f"Formatage des r√©sultats ({len(results['ids'][0])} documents trouv√©s)"
        )
        formatted_results = []
        seen_originals = set()  # Pour suivre les contenus originaux d√©j√† vus

        for i in range(len(results["ids"][0])):
            metadata = results["metadatas"][0][i]
            is_summary = metadata.get("is_summary", "false").lower() == "true"
            original_content = metadata.get("original_content", "")

            # Si c'est un r√©sum√©, on ajoute le contenu original aux m√©tadonn√©es pour r√©f√©rence
            if is_summary:
                result = {
                    "id": results["ids"][0][i],
                    "document": results["documents"][0][i],
                    "metadata": metadata,
                    "distance": results["distances"][0][i],
                    "is_summary": True,
                    "original_content": original_content
                }
                formatted_results.append(result)
                seen_originals.add(original_content)

            # Si c'est un contenu original et qu'on n'a pas d√©j√† son r√©sum√©
            elif results["documents"][0][i] not in seen_originals:
                result = {
                    "id": results["ids"][0][i],
                    "document": results["documents"][0][i],
                    "metadata": metadata,
                    "distance": results["distances"][0][i],
                    "is_summary": False
                }
                formatted_results.append(result)

        # Trier par score de similarit√© et limiter au nombre demand√©
        formatted_results.sort(key=lambda x: x["distance"])
        formatted_results = formatted_results[:n_results]

        logger.info(f"Recherche termin√©e, {len(formatted_results)} r√©sultats")
        # Log des statistiques sur les r√©sum√©s vs originaux
        summaries = sum(1 for r in formatted_results if r.get("is_summary", False))
        logger.info(f"R√©partition : {summaries} r√©sum√©s, {len(formatted_results) - summaries} originaux")
        
        return formatted_results

    def delete_collection(self) -> None:
        """Delete the current collection"""
        logger.warning(f"Suppression de la collection '{self.collection.name}'")
        self.client.delete_collection(self.collection.name)
        logger.info(f"Collection '{self.collection.name}' supprim√©e")

    def reset(self) -> None:
        """Reset the database and recreate the collection"""
        logger.warning("R√©initialisation compl√®te de la base de donn√©es ChromaDB")
        try:
            # Delete the collection if it exists
            try:
                self.client.delete_collection(self.collection.name)
            except Exception as e:
                logger.debug(f"Collection deletion failed (might not exist): {e}")
            
            # Recreate the collection
            self.collection = self.client.create_collection(
                name=self.collection.name,
                metadata={"hnsw:space": "cosine"}
            )
            logger.info("Base de donn√©es r√©initialis√©e et collection recr√©√©e")
        except Exception as e:
            logger.error(f"Error during reset: {e}")
            # If something went wrong, try to ensure we have a valid collection
            self.collection = self.client.get_or_create_collection(
                name=self.collection.name,
                metadata={"hnsw:space": "cosine"}
            )

    def get_all_documents(self) -> Dict:
        """
        R√©cup√®re tous les documents de la base de donn√©es avec leurs m√©tadonn√©es

        Returns:
            Dict: Dictionnaire avec les IDs des documents comme cl√©s et leurs m√©tadonn√©es comme valeurs
        """
        logger.debug("R√©cup√©ration de tous les documents")

        try:
            # R√©cup√©rer tous les documents
            all_docs = self.collection.get()

            if not all_docs or "metadatas" not in all_docs or not all_docs["metadatas"]:
                logger.debug("Aucun document trouv√© dans la base de donn√©es")
                return {}

            # Cr√©er un dictionnaire avec les ID comme cl√©s et les m√©tadonn√©es comme valeurs
            result = {}
            for i, doc_id in enumerate(all_docs["ids"]):
                result[doc_id] = all_docs["metadatas"][i]

            logger.debug(f"R√©cup√©ration termin√©e - {len(result)} documents trouv√©s")
            return result

        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des documents: {e}")
            return {}

def print_chunks_with_dates(chunks):
    print("\nüìù Affichage des chunks avec dates:")
    print("=" * 80)
    for i, chunk in enumerate(chunks, 1):
        print(f"\nChunk {i}/{len(chunks)}")
        print("-" * 40)
        print(f"Document: {chunk.get('metadata', {}).get('document_title', 'Sans titre')}")
        print(f"Section: {chunk.get('metadata', {}).get('section_number', 'Non sp√©cifi√©e')}")
        
        # Afficher les dates si pr√©sentes
        dates = chunk.get('metadata', {}).get('dates', [])
        if dates:
            print(f"\nüìÖ Dates d√©tect√©es: {', '.join(dates)}")
        
        print("\nContenu:")
        print(chunk.get('content', ''))
        print("-" * 40)
