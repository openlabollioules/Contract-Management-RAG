import os
import platform
from pathlib import Path
from typing import List

import numpy as np
import torch
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

from utils.logger import setup_logger

# Load environment variables
load_dotenv("config.env")

# Configurer le logger pour ce module
logger = setup_logger(__file__)


class TextVectorizer:
    def __init__(
        self,
        model_name: str = None,
        cache_dir: str = None,
    ):
        # Charger les variables d'environnement
        load_dotenv("config.env")

        # Utiliser les valeurs de config.env ou les valeurs par d√©faut
        self.model_name = model_name or os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
        self.cache_dir = Path(
            cache_dir or os.getenv("CACHE_DIR", "offline_models/embeddings_cache")
        )
        self.models_dir = Path(os.getenv("MODELS_DIR", "offline_models/hf"))
        self.embeddings_dir = Path(
            os.getenv("EMBEDDINGS_DIR", "offline_models/embeddings")
        )
        self.use_offline_models = (
            os.getenv("USE_OFFLINE_MODELS", "true").lower() == "true"
        )

        # Cr√©er les dossiers n√©cessaires
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.embeddings_dir.mkdir(parents=True, exist_ok=True)

        logger.info(
            f"Initialisation du TextVectorizer avec le mod√®le '{self.model_name}'"
        )
        logger.debug(f"R√©pertoire de cache: {self.cache_dir}")
        logger.debug(f"R√©pertoire de mod√®les: {self.models_dir}")
        logger.debug(f"Mode hors ligne: {self.use_offline_models}")

        # D√©tection du device
        is_apple_silicon = (
            platform.processor() == "arm" and platform.system() == "Darwin"
        )
        use_mps = os.getenv("USE_MPS", "true").lower() == "true"

        if is_apple_silicon and torch.backends.mps.is_available() and use_mps:
            self.device = torch.device("mps")
            logger.info("üéÆ Using MPS (Metal Performance Shaders) for embeddings")
        else:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"üíª Using {self.device} for embeddings")

        # Charger le mod√®le avec gestion des erreurs et fallback
        self._load_model()

    def _load_model(self):
        """Charge le mod√®le avec gestion des erreurs et fallback"""
        # Sauvegarde des variables d'environnement originales
        original_env = {}
        env_vars = ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE", "HF_DATASETS_OFFLINE"]
        for var in env_vars:
            original_env[var] = os.environ.get(var)

        try:
            # Essayer d'abord en mode hors ligne si demand√©
            if self.use_offline_models:
                logger.info(
                    "üîí Mode hors ligne activ√© - Tentative d'utilisation des mod√®les locaux"
                )
                # Configurer l'environnement pour le mode hors ligne
                os.environ["HF_HUB_OFFLINE"] = "1"
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_DATASETS_OFFLINE"] = "1"
                os.environ["NO_PROXY"] = "*"
                os.environ["http_proxy"] = ""
                os.environ["https_proxy"] = ""

                try:
                    logger.info(
                        f"üîÑ Chargement du mod√®le {self.model_name} depuis {self.models_dir} (mode hors ligne)"
                    )
                    self.model = SentenceTransformer(
                        self.model_name,
                        cache_folder=str(self.models_dir),
                        local_files_only=True,
                    )
                    logger.info(f"‚úÖ Mod√®le charg√© avec succ√®s en mode hors ligne")
                except Exception as e:
                    logger.warning(
                        f"‚ö†Ô∏è Impossible de charger le mod√®le en mode hors ligne: {str(e)}"
                    )
                    logger.info(
                        "üåê Passage en mode en ligne pour t√©l√©charger le mod√®le"
                    )

                    # R√©initialiser les variables d'environnement pour passer en mode en ligne
                    for var in env_vars:
                        if original_env[var] is None:
                            if var in os.environ:
                                del os.environ[var]
                        else:
                            os.environ[var] = original_env[var]

                    # T√©l√©charger le mod√®le en mode en ligne
                    logger.info(
                        f"üì• T√©l√©chargement du mod√®le {self.model_name} vers {self.models_dir}"
                    )
                    self.model = SentenceTransformer(
                        self.model_name, cache_folder=str(self.models_dir)
                    )
                    logger.info(f"‚úÖ Mod√®le t√©l√©charg√© avec succ√®s")

                    # Recommander √† l'utilisateur d'ex√©cuter la fonction de t√©l√©chargement
                    logger.info(
                        "üí° Pour utiliser ce mod√®le en mode hors ligne √† l'avenir, ex√©cutez download_models_for_offline_use()"
                    )
            else:
                # Chargement normal en mode en ligne
                logger.info(
                    f"üåê Chargement du mod√®le {self.model_name} en mode en ligne"
                )
                self.model = SentenceTransformer(
                    self.model_name, cache_folder=str(self.models_dir)
                )
                logger.info(f"‚úÖ Mod√®le charg√© avec succ√®s")

            # D√©placer le mod√®le sur le bon device
            self.model.to(self.device)
            logger.info(f"‚úÖ Mod√®le d√©plac√© vers {self.device}")

            # Optimisations pour MPS/CUDA
            use_half_precision = (
                os.getenv("USE_HALF_PRECISION", "true").lower() == "true"
            )
            if use_half_precision and (
                self.device.type == "mps" or self.device.type == "cuda"
            ):
                self.model.half()  # Utiliser la pr√©cision FP16 pour de meilleures performances
                logger.debug(f"‚ö° Optimisation FP16 appliqu√©e pour {self.device.type}")

        except Exception as e:
            logger.error(
                f"‚ùå Erreur fatale lors du chargement du mod√®le {self.model_name}: {str(e)}"
            )
            raise
        finally:
            # Restaurer les variables d'environnement originales si le mode hors ligne est activ√©
            if self.use_offline_models:
                for var in env_vars:
                    if original_env[var] is None:
                        if var in os.environ:
                            del os.environ[var]
                    else:
                        os.environ[var] = original_env[var]

    @classmethod
    def download_models_for_offline_use(cls, model_name=None, all_models=True):
        """T√©l√©charge les mod√®les pour une utilisation hors ligne

        T√©l√©charge et met en cache les mod√®les d'embeddings pour une utilisation future
        sans connexion internet. Supporte deux mod√®les principaux :
        
        1. sentence-transformers/all-mpnet-base-v2 :
           - Contexte: 768 tokens
           - Pr√©cision: Excellente pour les textes courts
           - Utilis√© pour le chunking s√©mantique
        
        2. BAAI/bge-m3 :  
           - Contexte: Peut g√©rer des textes plus longs (1024+)
           - Pr√©cision: Sup√©rieure √† all-mpnet-base-v2 sur les textes juridiques longs
           - Utilis√© pour l'embedding de documents

        Args:
            model_name: Nom du mod√®le √† t√©l√©charger (optionnel)
            all_models: Si True, t√©l√©charge aussi les autres mod√®les utilis√©s dans l'application
        """
        # Charger les variables d'environnement
        load_dotenv("config.env")

        # Liste des mod√®les √† t√©l√©charger
        models_to_download = []

        # Le mod√®le principal (embeddings)
        main_model = model_name or os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
        models_to_download.append(main_model)

        # Ajouter les autres mod√®les utilis√©s dans l'application
        if all_models:
            # Mod√®le utilis√© par TextChunker
            models_to_download.append("sentence-transformers/all-mpnet-base-v2")
            # Ajouter d'autres mod√®les ici si n√©cessaire

        # R√©pertoire des mod√®les
        models_dir = Path(os.getenv("MODELS_DIR", "offline_models/hf"))
        models_dir.mkdir(parents=True, exist_ok=True)

        # D√©sactiver temporairement le mode hors ligne
        original_env = {}
        env_vars = ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE", "HF_DATASETS_OFFLINE"]
        for var in env_vars:
            original_env[var] = os.environ.get(var)
            if var in os.environ:
                del os.environ[var]

        # R√©sultats du t√©l√©chargement
        results = {}

        try:
            # T√©l√©charger chaque mod√®le
            for model in models_to_download:
                logger.info(
                    f"üì• T√©l√©chargement du mod√®le {model} pour utilisation hors ligne"
                )
                logger.info(f"Destination: {models_dir}")

                try:
                    # T√©l√©charger le mod√®le
                    model_instance = SentenceTransformer(
                        model, cache_folder=str(models_dir)
                    )
                    # Forcer le t√©l√©chargement du tokenizer et des fichiers de configuration
                    _ = model_instance.tokenizer

                    # Acc√®s √† d'autres composants selon le type de mod√®le
                    try:
                        _ = model_instance.auto_model.config
                    except AttributeError:
                        # Certains mod√®les n'ont pas d'attribut auto_model
                        logger.debug(
                            f"Le mod√®le {model} n'a pas d'attribut auto_model, ignor√©"
                        )

                    logger.info(f"‚úÖ Mod√®le {model} t√©l√©charg√© avec succ√®s")
                    results[model] = True
                except Exception as e:
                    logger.error(
                        f"‚ùå Erreur lors du t√©l√©chargement du mod√®le {model}: {str(e)}"
                    )
                    results[model] = False

            # Message de succ√®s global
            success_count = sum(1 for v in results.values() if v)
            if success_count == len(models_to_download):
                logger.info(
                    f"‚úÖ Tous les mod√®les ({len(models_to_download)}) ont √©t√© t√©l√©charg√©s avec succ√®s"
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è {success_count}/{len(models_to_download)} mod√®les t√©l√©charg√©s avec succ√®s"
                )

            logger.info(
                f"üí° Pour utiliser ces mod√®les, assurez-vous que USE_OFFLINE_MODELS=true dans config.env"
            )
            return results
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du t√©l√©chargement des mod√®les: {str(e)}")
            return results
        finally:
            # Restaurer les variables d'environnement originales
            for var in env_vars:
                if original_env[var] is not None:
                    os.environ[var] = original_env[var]

    def _get_cache_path(self, text: str) -> Path:
        """G√©n√®re un chemin de cache unique pour le texte"""
        # Utiliser un hash du texte comme nom de fichier
        text_hash = str(hash(text))
        cache_path = self.cache_dir / f"{text_hash}.npy"
        logger.debug(f"Chemin de cache g√©n√©r√©: {cache_path}")
        return cache_path

    def _load_from_cache(self, cache_path: Path) -> np.ndarray:
        """Charge les embeddings depuis le cache"""
        logger.debug(f"Chargement des embeddings depuis le cache: {cache_path}")
        return np.load(str(cache_path))

    def _save_to_cache(self, embeddings: np.ndarray, cache_path: Path):
        """Sauvegarde les embeddings dans le cache"""
        logger.debug(f"Sauvegarde des embeddings dans le cache: {cache_path}")
        np.save(str(cache_path), embeddings)

    def get_embeddings(
        self, texts: List[str], use_cache: bool = True
    ) -> List[np.ndarray]:
        """G√©n√®re ou r√©cup√®re les embeddings pour une liste de textes"""
        logger.info(
            f"Demande d'embeddings pour {len(texts)} textes (use_cache={use_cache})"
        )
        results = []
        texts_to_embed = []
        cache_paths = []

        # V√©rifier le cache pour chaque texte
        for text in texts:
            cache_path = self._get_cache_path(text)
            if use_cache and cache_path.exists():
                logger.debug(f"Embeddings trouv√©s dans le cache")
                results.append(self._load_from_cache(cache_path))
            else:
                logger.debug(
                    f"Embeddings non trouv√©s dans le cache, ajout √† la liste √† calculer"
                )
                texts_to_embed.append(text)
                cache_paths.append(cache_path)

        # G√©n√©rer les nouveaux embeddings si n√©cessaire
        if texts_to_embed:
            logger.info(f"G√©n√©ration de {len(texts_to_embed)} nouveaux embeddings")
            with torch.no_grad():
                embeddings = self.model.encode(
                    texts_to_embed,
                    batch_size=32,
                    show_progress_bar=True,
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                )
                logger.debug(f"Embeddings g√©n√©r√©s avec succ√®s")

            # Sauvegarder dans le cache et ajouter aux r√©sultats
            for emb, cache_path in zip(embeddings, cache_paths):
                self._save_to_cache(emb, cache_path)
                results.append(emb)

        logger.info(f"Retour de {len(results)} embeddings au total")
        return results

    def compute_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """Calcule la similarit√© cosinus entre deux embeddings"""
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        logger.debug(f"Similarit√© calcul√©e: {similarity:.4f}")
        return similarity
