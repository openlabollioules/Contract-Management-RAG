from dataclasses import dataclass

from utils.logger import setup_logger

from .embeddings_manager import EmbeddingsManager
from .intelligent_splitter import ChunkMetadata, ChunkType

# Configurer le logger pour ce module
logger = setup_logger(__file__)


@dataclass
class OptimizedChunk:
    """ReprÃ©sente un chunk optimisÃ© avec ses mÃ©tadonnÃ©es"""

    content: str
    metadata: ChunkMetadata
    chunk_type: ChunkType
    similarity_score: float = 0.0
    is_merged: bool = False


class ChunkOptimizer:
    """Optimise les chunks en utilisant la similaritÃ© sÃ©mantique et le Text Tiling"""

    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.embeddings_manager = EmbeddingsManager()
        logger.info(
            f"ChunkOptimizer initialisÃ© avec similarity_threshold={similarity_threshold}"
        )
        logger.debug("EmbeddingsManager crÃ©Ã© pour ChunkOptimizer")

    """def optimize_chunks(self, chunks: List[Chunk]) -> List[Chunk]:
        print("\nğŸ” Optimisation des chunks...")
        start_time = time.time()
        
        # Convertir les chunks en OptimizedChunk
        optimized_chunks = [
            OptimizedChunk(
                content=chunk.content,
                metadata=chunk.metadata,
                chunk_type=chunk.chunk_type
            )
            for chunk in chunks
        ]
        
        # GÃ©nÃ©rer les embeddings pour tous les chunks
        print("ğŸ“Š GÃ©nÃ©ration des embeddings...")
        chunk_embeddings = self.embeddings_manager.get_embeddings(
            [chunk.content for chunk in optimized_chunks]
        )
        
        # Calculer les similaritÃ©s entre chunks consÃ©cutifs
        print("ğŸ” Calcul des similaritÃ©s...")
        for i in tqdm(range(len(optimized_chunks) - 1), desc="Analyse des similaritÃ©s"):
            similarity = self.embeddings_manager.compute_similarity(
                chunk_embeddings[i],
                chunk_embeddings[i + 1]
            )
            
            # Si la similaritÃ© est Ã©levÃ©e, marquer pour fusion
            if similarity > self.similarity_threshold:
                optimized_chunks[i].is_merged = True
                optimized_chunks[i + 1].is_merged = True
                optimized_chunks[i].similarity_score = similarity
        
        # Fusionner les chunks marquÃ©s
        print("ğŸ”„ Fusion des chunks similaires...")
        merged_chunks = []
        current_chunk = None
        
        for chunk in optimized_chunks:
            if not chunk.is_merged:
                if current_chunk:
                    merged_chunks.append(current_chunk)
                current_chunk = Chunk(
                    content=chunk.content,
                    metadata=chunk.metadata,
                    chunk_type=chunk.chunk_type
                )
            else:
                if current_chunk:
                    # Fusionner le contenu
                    current_chunk.content += "\n" + chunk.content
                    # Mettre Ã  jour les mÃ©tadonnÃ©es
                    current_chunk.metadata.section_hierarchy.extend(chunk.metadata.section_hierarchy)
                    if chunk.metadata.section_title:
                        current_chunk.metadata.section_title = chunk.metadata.section_title
                else:
                    current_chunk = Chunk(
                        content=chunk.content,
                        metadata=chunk.metadata,
                        chunk_type=chunk.chunk_type
                    )
        
        if current_chunk:
            merged_chunks.append(current_chunk)
            
        print(f"\nâœ… Optimisation terminÃ©e en {time.time() - start_time:.2f} secondes")
        print(f"ğŸ“Š RÃ©sultats:")
        print(f"  - Chunks initiaux: {len(chunks)}")
        print(f"  - Chunks optimisÃ©s: {len(merged_chunks)}")
        print(f"  - RÃ©duction: {((len(chunks) - len(merged_chunks)) / len(chunks) * 100):.1f}%")
        
        return merged_chunks"""
