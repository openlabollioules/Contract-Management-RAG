import re
import sys
import time
import networkx as nx
import matplotlib.pyplot as plt
from typing import List

from rag.chroma_manager import ChromaDBManager
from rag.embeddings_manager import EmbeddingsManager
from rag.hierarchical_grouper import HierarchicalGrouper
from rag.intelligent_splitter import Chunk, IntelligentSplitter
from rag.pdf_loader import extract_text_contract
from rag.semantic_chunker import SemanticChunkManager
from rag.graph_rag import GraphRAG


def display_chunks_details(chunks: List[Chunk]) -> None:
    """
    Affiche le contenu d√©taill√© de chaque chunk avec ses m√©tadonn√©es

    Args:
        chunks: Liste des chunks √† afficher
    """
    print("\nüìã D√©tails des chunks:")
    print("=" * 80)

    for i, chunk in enumerate(chunks, 1):
        print(f"\nChunk {i}/{len(chunks)}")
        print("-" * 40)
        print(f"Section: {chunk.section_number}")
        print(f"Hi√©rarchie: {' -> '.join(chunk.hierarchy)}")
        print(f"Document: {chunk.document_title}")
        print(f"Chapitre: {chunk.chapter_title}")
        print(f"Section parente: {chunk.parent_section}")
        print(
            f"Position: {getattr(chunk, 'position', 'N/A')}/{getattr(chunk, 'total_chunks', 'N/A')}"
        )
        print(f"Taille (mots): {len(chunk.content.split())}")
        print("\nContenu:")
        print(chunk.content)
        print("-" * 40)


def display_removed_content(original_text: str, chunks: List[Chunk]) -> None:
    """
    Affiche TOUTES les lignes du texte original qui ne sont pas pr√©sentes dans les chunks.
    Utilise une comparaison ligne par ligne stricte.
    
    Args:
        original_text: Texte original du document
        chunks: Liste des chunks apr√®s d√©coupage
    """
    print("\nüóëÔ∏è Contenu supprim√© lors du d√©coupage:")
    print("=" * 80)
    
    # Extraire toutes les lignes du texte original, avec normalisation minimale
    original_lines = []
    for line in original_text.split("\n"):
        line = line.strip()
        if line:  # Ignorer les lignes vides
            original_lines.append(line)
    
    # Extraire toutes les lignes des chunks
    chunk_lines = []
    for chunk in chunks:
        for line in chunk.content.split("\n"):
            line = line.strip()
            if line:  # Ignorer les lignes vides
                chunk_lines.append(line)
    
    # Liste tr√®s restrictive de patterns qui correspondent uniquement √† des titres
    # Ces patterns doivent √™tre strictement limit√©s aux formats de titres courants dans les contrats
    strict_title_patterns = [
        # Titres markdown (# Titre)
        r"^#+\s+\**[A-Z][A-Z\s]+\**$",
        # Formats explicites de sections num√©rot√©es
        r"^ARTICLE\s+[IVXLCDM]+\s*[:\.]",
        r"^SECTION\s+[0-9]+\s*[:\.]",
        r"^CHAPTER\s+[0-9]+\s*[:\.]",
        r"^ANNEXE?\s+[A-Z]:\s*",
        r"^APPENDIX\s+[A-Z]:\s*",
        # Titres num√©rot√©s tout en majuscules (tr√®s sp√©cifique)
        r"^[0-9]+\.\s+[A-Z][A-Z\s]+$",
        # Nouveaux patterns plus pr√©cis pour les titres de contrats
        r"^#+\s+\**\d+\.\d+\s+[A-Z][A-Z\s]+\**$",  # ## 1.2 TITLE
        r"^#+\s+\**\d+\.\d+\.\d+\s+[A-Z][A-Z\s]+\**$",  # ## 1.2.3 TITLE
        r"^\*\*[A-Z][A-Z\s]+\*\*$",  # **TITLE**
        r"^[A-Z][A-Z\s]+:$",  # TITLE:
    ]
    
    # Fonction pour v√©rifier si une ligne est un titre
    def is_strict_title(line):
        # V√©rifier les patterns explicites de titre
        for pattern in strict_title_patterns:
            if re.match(pattern, line):
                return True
                
        # V√©rifier le cas sp√©cial pour "# N. TITLE"
        if re.match(r"^#+\s+\**[0-9]+\.\s+[A-Z][A-Z\s]+\**$", line):
            return True
            
        # Si le texte est court (<5 mots), tout en majuscules et pas de ponctuation finale,
        # c'est probablement un titre
        words = line.split()
        if (
            len(words) <= 5
            and line.isupper()
            and not line.endswith((".", "!", "?", ",", ";", ":", ")", "]"))
        ):
            return True
            
        return False
    
    # Comparer chaque ligne originale avec les lignes des chunks
    # Une ligne est consid√©r√©e pr√©sente si elle est exactement dans les chunks (ou tr√®s l√©g√®rement diff√©rente)
    def is_line_present(line, chunk_lines):
        # Normalisation minimale
        def normalize_for_comparison(text):
            # Supprime juste les espaces en d√©but/fin et r√©duit les espaces multiples
            return re.sub(r"\s+", " ", text).strip()
        
        normalized_line = normalize_for_comparison(line)
        
        # V√©rification exacte
        for chunk_line in chunk_lines:
            normalized_chunk_line = normalize_for_comparison(chunk_line)
            if normalized_line == normalized_chunk_line:
                return True
            
            # V√©rification avec l√©g√®re tol√©rance pour les espaces/tirets/points
            # Remplacer les caract√®res sp√©ciaux par des espaces et comparer
            clean_line = re.sub(r"[-_.,;:()]", " ", normalized_line)
            clean_line = re.sub(r"\s+", " ", clean_line).strip()
            
            clean_chunk = re.sub(r"[-_.,;:()]", " ", normalized_chunk_line)
            clean_chunk = re.sub(r"\s+", " ", clean_chunk).strip()
            
            if clean_line == clean_chunk:
                return True
                
        return False
    
    # Trouver les lignes qui ne sont pas dans les chunks
    missing_titles = []
    missing_content = []
    
    for i, line in enumerate(original_lines):
        if not is_line_present(line, chunk_lines):
            # Collecter le contexte (ligne pr√©c√©dente et suivante)
            context = []
            if i > 0:
                context.append(f"Ligne pr√©c√©dente: {original_lines[i-1]}")
            
            if i < len(original_lines) - 1:
                context.append(f"Ligne suivante: {original_lines[i+1]}")
            
            # V√©rifier si c'est un titre
            if is_strict_title(line):
                context.append(f"TITRE supprim√©: {line}")
                missing_titles.append((context, line))
            else:
                context.append(f"LIGNE supprim√©e: {line}")
                missing_content.append((context, line))
    
    # Afficher les titres supprim√©s
    if missing_titles:
        print("\nüìë Titres supprim√©s:")
        print("-" * 40)
        for context, title in missing_titles:
            for line in context:
                print(f"- {line}")
            print("-" * 40)
    
    # Afficher les lignes de contenu supprim√©es
    if missing_content:
        print(f"\nüìÑ {len(missing_content)} lignes supprim√©es")
        print("\n‚ö†Ô∏è D√©tail des lignes supprim√©es:")
        print("-" * 40)
        for context, content in missing_content:
            for line in context:
                print(f"- {line}")
            print("-" * 40)
    
    # Statistiques
    print(f"\nüìä Statistiques du traitement:")
    print(f"- Nombre total de titres supprim√©s: {len(missing_titles)}")
    print(f"- Nombre total de lignes supprim√©es: {len(missing_content)}")
    
    if not missing_titles and not missing_content:
        print("Aucune ligne n'a √©t√© supprim√©e lors du d√©coupage.")


def display_semantic_split_chunks(
    structure_chunks: List[Chunk], final_chunks: List[Chunk]
) -> None:
    """
    Affiche les chunks qui ont subi un d√©coupage s√©mantique dans l'approche hybride

    Args:
        structure_chunks: Chunks initiaux apr√®s d√©coupage structurel
        final_chunks: Chunks finaux apr√®s d√©coupage s√©mantique
    """
    print("\nüîç Chunks ayant subi un d√©coupage s√©mantique:")
    print("=" * 80)

    # Identifier les chunks originaux qui ont √©t√© d√©coup√©s
    split_chunks = []
    for original_chunk in structure_chunks:
        # Compter combien de chunks finaux proviennent de ce chunk original
        sub_chunks = [
            c for c in final_chunks if c.section_number == original_chunk.section_number
        ]
        if len(sub_chunks) > 1:  # Si le chunk a √©t√© d√©coup√©
            split_chunks.append((original_chunk, sub_chunks))

    if not split_chunks:
        print("Aucun chunk n'a subi de d√©coupage s√©mantique.")
        return

    print(f"Nombre de chunks d√©coup√©s s√©mantiquement: {len(split_chunks)}")

    for original_chunk, sub_chunks in split_chunks:
        print("\n" + "=" * 40)
        print(f"Chunk original:")
        print(f"Section: {original_chunk.section_number}")
        print(f"Hi√©rarchie: {' -> '.join(original_chunk.hierarchy)}")
        print(f"Taille originale: {len(original_chunk.content.split())} mots")
        print("\nD√©coup√© en {len(sub_chunks)} sous-chunks:")

        for i, sub_chunk in enumerate(sub_chunks, 1):
            print(f"\nSous-chunk {i}/{len(sub_chunks)}:")
            print(
                f"Position: {getattr(sub_chunk, 'position', 'N/A')}/{getattr(sub_chunk, 'total_chunks', 'N/A')}"
            )
            print(f"Taille: {len(sub_chunk.content.split())} mots")
            print(
                f"Contenu: {sub_chunk.content[:200]}..."
            )  # Afficher les 200 premiers caract√®res
        print("=" * 40)


def process_contract(filepath: str) -> List[Chunk]:
    """
    Process a contract file and return intelligent chunks using a hybrid approach:
    1. First split by legal structure (articles, sections, subsections)
    2. Then apply semantic chunking for sections exceeding 800 tokens
    3. Preserve hierarchical metadata for traceability
    4. Apply post-processing to restore important legal content that might have been lost
    5. Build a document graph to enhance retrieval through structural relationships

    Args:
        filepath: Path to the contract file

    Returns:
        List of Chunk objects with preserved legal structure and metadata
    """
    print("\nüîÑ D√©but du traitement du document...")
    start_time = time.time()

    # 1. Load and extract text from PDF
    print(
        "üìÑ Extraction du texte du PDF (avec d√©tection des en-t√™tes/pieds de page et suppression des r√©f√©rences d'images)..."
    )
    text, document_title = extract_text_contract(filepath)
    print(f"‚úÖ Texte extrait ({len(text.split())} mots)")

    print("\nüîÑ D√©coupage du texte avec approche hybride (structure + s√©mantique)...")
    # First split by legal structure
    splitter = IntelligentSplitter(document_title=document_title)
    structure_chunks = splitter.split(text)
    
    # Then apply semantic chunking for large sections
    semantic_manager = SemanticChunkManager(
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=0.6,
        buffer_size=3,
        chunk_size=800,  # Limite de ~800 tokens
        chunk_overlap=100,  # Chevauchement de 100 tokens
    )
    
    chunks = []
    for chunk in structure_chunks:
        # If section is small enough, keep it as is
        if len(chunk.content.split()) < 800:  # Approximate token count
            chunks.append(chunk)
        else:
            # For large sections, apply semantic chunking
            sub_chunks = semantic_manager.chunk_text(chunk.content)
            # Preserve metadata in sub-chunks
            for sub_chunk in sub_chunks:
                sub_chunk.section_number = chunk.section_number
                sub_chunk.hierarchy = chunk.hierarchy
                sub_chunk.document_title = chunk.document_title
                sub_chunk.parent_section = chunk.parent_section
                sub_chunk.chapter_title = chunk.chapter_title
                # Add position metadata
                sub_chunk.position = len(chunks)
                sub_chunk.total_chunks = len(sub_chunks)
            chunks.extend(sub_chunks)

    # 2.5 Post-traitement: restaurer le contenu juridique important qui aurait pu √™tre perdu
    print("\nüîÑ Application du post-traitement pour restaurer le contenu juridique important...")
    chunks = restore_important_content(text, chunks)

    # 3. Group chunks hierarchically
    print("\nüîç Regroupement hi√©rarchique des chunks...")
    grouper = HierarchicalGrouper()
    hierarchical_groups = grouper.group_chunks(chunks)

    # 4. Initialize embeddings and ChromaDB
    print("\nüîç Initialisation des embeddings et de ChromaDB...")
    embeddings_manager = EmbeddingsManager()
    chroma_manager = ChromaDBManager(embeddings_manager)

    # 5. Prepare chunks for ChromaDB with enhanced metadata
    print("\nüì¶ Pr√©paration des chunks pour ChromaDB...")
    chroma_chunks = []
    for chunk in chunks:
        # Enhanced metadata structure
        metadata = {
            "section_number": chunk.section_number or "unknown",
            "hierarchy": chunk.hierarchy or ["unknown"],
            "document_title": chunk.document_title or "unknown",
            "parent_section": chunk.parent_section or "unknown",
            "chapter_title": chunk.chapter_title or "unknown",
            "title": document_title,
            "content": chunk.content,
            "chunk_type": (
                str(chunk.chunk_type) if hasattr(chunk, "chunk_type") else "unknown"
            ),
            "position": getattr(chunk, "position", None),
            "total_chunks": getattr(chunk, "total_chunks", None),
            "chunk_size": len(chunk.content.split()),  # Approximate token count
            "timestamp": time.time(),
        }

        # Enhanced content with metadata
        content = f"""
Section: {metadata['section_number']}
Hi√©rarchie compl√®te: {' -> '.join(metadata['hierarchy'])}
Document: {metadata['document_title']}
Position: {metadata['position']}/{metadata['total_chunks']}

Contenu:
{chunk.content}
"""
        chroma_chunks.append({"content": content, "metadata": metadata})

    # 6. Add chunks to ChromaDB
    print("\nüíæ Ajout des chunks √† ChromaDB...")
    chroma_manager.add_documents(chroma_chunks)
    print("‚úÖ Chunks ajout√©s √† ChromaDB")

    # 7. Build the document graph
    print("\nüîç Construction du graphe du document...")
    build_and_analyze_document_graph(chroma_manager, embeddings_manager)

    # Print document metadata
    print("\nDocument Metadata:")
    print(f"- Title: {document_title}")
    print(f"- Author: Unknown")
    print(f"- Pages: Unknown")
    
    # Print processing time and statistics
    processing_time = time.time() - start_time
    print(f"\n‚è±Ô∏è Temps total de traitement: {processing_time:.2f} secondes")

    print(f"üìä Nombre de chunks cr√©√©s: {len(chunks)}")
    print(
        f"üìä Taille moyenne des chunks: {sum(len(c.content.split()) for c in chunks) / len(chunks):.1f} tokens"
    )

    # Display chunks details and removed content
    display_chunks_details(chunks)
    display_removed_content(text, chunks)
    
    # Display semantic split chunks if in hybrid mode
    if structure_chunks:
        display_semantic_split_chunks(structure_chunks, chunks)

    return chunks


def build_and_analyze_document_graph(chroma_manager: ChromaDBManager, embeddings_manager: EmbeddingsManager) -> None:
    """
    Build and analyze the document graph, with detailed logging.
    
    Args:
        chroma_manager: ChromaDB manager with document chunks
        embeddings_manager: Embeddings manager for the graph
    """
    # Initialize GraphRAG
    print("üîÑ Initialisation de GraphRAG...")
    graph_rag = GraphRAG(
        embeddings_manager=embeddings_manager,
        chroma_manager=chroma_manager
    )
    
    # If no graph was built, log the issue
    if not graph_rag.graph or len(graph_rag.graph.nodes) == 0:
        print("‚ö†Ô∏è Aucun graphe n'a √©t√© construit. V√©rification des m√©tadonn√©es...")
        
        # Check document metadata to diagnose issues
        results = chroma_manager.collection.get(include=["metadatas"])
        if not results["metadatas"]:
            print("‚ùå Aucun document n'a √©t√© trouv√© dans ChromaDB.")
            return
            
        # Check key metadata fields
        section_numbers = [m.get("section_number") for m in results["metadatas"]]
        hierarchies = [m.get("hierarchy") for m in results["metadatas"]]
        
        valid_sections = sum(1 for s in section_numbers if s and s != "unknown")
        valid_hierarchies = sum(1 for h in hierarchies if h and h != ["unknown"])
        
        print(f"üìä Documents dans ChromaDB: {len(results['metadatas'])}")
        print(f"üìä Documents avec section valide: {valid_sections} ({valid_sections/len(results['metadatas'])*100:.1f}%)")
        print(f"üìä Documents avec hi√©rarchie valide: {valid_hierarchies} ({valid_hierarchies/len(results['metadatas'])*100:.1f}%)")
        
        # Show sample metadata for debugging
        print("\nüìã √âchantillon de m√©tadonn√©es:")
        for i, meta in enumerate(results["metadatas"][:3]):
            print(f"\nDocument {i+1}:")
            print(f"- Section: {meta.get('section_number', 'Non sp√©cifi√©')}")
            print(f"- Hi√©rarchie: {meta.get('hierarchy', 'Non sp√©cifi√©')}")
            print(f"- Document: {meta.get('document_title', 'Non sp√©cifi√©')}")
            print(f"- Section parente: {meta.get('parent_section', 'Non sp√©cifi√©')}")
        
        print("\n‚ö†Ô∏è Recommandations pour am√©liorer le graphe:")
        print("1. V√©rifiez que votre document contient des num√©ros de section bien structur√©s")
        print("2. Utilisez l'option --semantic-chunking pour pr√©server la structure hi√©rarchique")
        print("3. Si n√©cessaire, modifiez IntelligentSplitter pour mieux d√©tecter la structure du document")
        return
        
    # Log graph information
    print("\nüìä Analyse du graphe construit:")
    print(f"- Nombre de n≈ìuds: {len(graph_rag.graph.nodes)}")
    print(f"- Nombre d'ar√™tes: {len(graph_rag.graph.edges)}")
    
    # Analyze graph structure
    degree_centrality = nx.degree_centrality(graph_rag.graph)
    if degree_centrality:
        most_central = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
        print("\nüìä Sections les plus centrales (avec le plus de connexions):")
        for node, centrality in most_central:
            print(f"- Section {node}: {centrality:.3f} ({graph_rag.graph.degree(node)} connexions)")
    
    # Check for isolated nodes
    isolated_nodes = list(nx.isolates(graph_rag.graph))
    if isolated_nodes:
        print(f"\n‚ö†Ô∏è {len(isolated_nodes)} n≈ìuds isol√©s (sans connexions)")
        print(f"Exemples: {isolated_nodes[:5]}")
    
    # Count edge types
    relation_counts = {}
    for _, _, data in graph_rag.graph.edges(data=True):
        relation = data.get("relation", "unknown")
        relation_counts[relation] = relation_counts.get(relation, 0) + 1
    
    print("\nüìä Types de relations dans le graphe:")
    for relation, count in relation_counts.items():
        print(f"- {relation}: {count} ar√™tes")
    
    # Overall graph quality assessment
    density = nx.density(graph_rag.graph)
    print(f"\nüìä Densit√© du graphe: {density:.4f}")
    
    if density < 0.01:
        print("‚ö†Ô∏è Graphe tr√®s peu dense. Les relations entre sections pourraient √™tre insuffisantes.")
    elif density > 0.5:
        print("‚ö†Ô∏è Graphe extr√™mement dense. Pourrait indiquer de nombreuses connexions non pertinentes.")
    else:
        print("‚úÖ Densit√© du graphe dans une plage raisonnable.")
    
    print("\n‚úÖ Construction et analyse du graphe termin√©es.")
    
    # Optionally save graph visualization
    try:
        print("\nüîÑ G√©n√©ration de la visualisation du graphe...")
        plt.figure(figsize=(12, 10))
        
        # Create a simplified graph for visualization if needed
        G = graph_rag.graph
        if len(G.nodes) > 50:
            print(f"‚ö†Ô∏è Le graphe est trop grand ({len(G.nodes)} n≈ìuds). Affichage limit√© √† 50 n≈ìuds.")
            top_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:50]
            nodes_to_show = [node for node, _ in top_nodes]
            G = G.subgraph(nodes_to_show)
        
        # Use different colors for different types of edges
        edge_colors = []
        edge_labels = {}
        
        for u, v, data in G.edges(data=True):
            relation = data.get("relation", "unknown")
            if relation == "next":
                edge_colors.append("blue")
            elif relation == "previous":
                edge_colors.append("green")
            elif relation == "references":
                edge_colors.append("red")
                edge_labels[(u, v)] = "ref"
            else:
                edge_colors.append("gray")
        
        # Position nodes using a hierarchical layout
        pos = nx.spring_layout(G, seed=42)
        
        # Draw nodes with size based on centrality
        node_sizes = [300 + 1000 * degree_centrality.get(node, 0) for node in G.nodes()]
        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color="lightblue", alpha=0.8)
        
        # Draw edges
        nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color=edge_colors, 
                              arrowsize=15, connectionstyle="arc3,rad=0.1")
        
        # Draw labels
        nx.draw_networkx_labels(G, pos, font_size=8, font_family="sans-serif")
        
        # Draw edge labels for references
        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)
        
        plt.title("Structure du graphe du document")
        plt.axis("off")
        plt.tight_layout()
        plt.savefig("document_graph.png", dpi=300, bbox_inches="tight")
        print("‚úÖ Visualisation du graphe sauvegard√©e dans 'document_graph.png'")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration de la visualisation: {str(e)}")


def search_contracts(query: str, n_results: int = 5, use_graph: bool = False, max_hop: int = 2) -> None:
    """
    Search in the contract database

    Args:
        query: Search query
        n_results: Number of results to return
        use_graph: Whether to use GraphRAG for enhanced retrieval
        max_hop: Maximum number of hops in graph traversal (only used with GraphRAG)
    """
    print(f"\nüîç Recherche: {query}")

    if use_graph:
        print("üìä Utilisation de GraphRAG pour la recherche...")
        graph_rag = GraphRAG()
        results = graph_rag.retrieve(query, n_results=n_results, max_hop=max_hop)
    else:
        # Initialize managers
        embeddings_manager = EmbeddingsManager()
        chroma_manager = ChromaDBManager(embeddings_manager)
        # Search
        results = chroma_manager.search(query, n_results=n_results)

    # Display results
    print(f"\nüìä R√©sultats ({len(results)} trouv√©s):")
    for i, result in enumerate(results, 1):
        print(f"\n--- R√©sultat {i} ---")
        print(f"Section: {result['metadata'].get('section_number', 'Non sp√©cifi√©')}")
        print(f"Hi√©rarchie: {result['metadata'].get('hierarchy', 'Non sp√©cifi√©')}")
        print(f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}")
        print(f"Contenu: {result['document'][:200]}...")
        print(f"Distance: {result.get('distance', 1.0):.4f}")
        if 'source' in result and result['source'] == 'graph':
            print(f"Source: D√©couvert via l'analyse graphique")


def chat_with_contract(query: str, n_context: int = 3, use_graph: bool = False, max_hop: int = 2) -> None:
    """
    Chat with the contract using embeddings for context and Ollama for generation

    Args:
        query: User's question
        n_context: Number of relevant chunks to use as context
        use_graph: Whether to use GraphRAG for enhanced retrieval
        max_hop: Maximum number of hops in graph traversal (only used with GraphRAG)
    """
    print(f"\nüí¨ Chat: {query}")

    if use_graph:
        print("üìä Utilisation de GraphRAG pour le chat...")
        graph_rag = GraphRAG()
        result_obj = graph_rag.chat_with_graph(query, n_context=n_context, max_hop=max_hop)
        response = result_obj["response"]
        results = result_obj["sources"]
    else:
        # Initialize managers
        embeddings_manager = EmbeddingsManager()
        chroma_manager = ChromaDBManager(embeddings_manager)

        # Search for relevant context
        results = chroma_manager.search(query, n_results=n_context)

        # Prepare context for the prompt
        context = "\n\n".join(
            [
                f"Document: {result['metadata'].get('document_title', 'Non sp√©cifi√©')}\n"
                f"Section: {result['metadata'].get('section_number', 'Non sp√©cifi√©')}\n"
                f"Chapter: {result['metadata'].get('chapter_title', 'Non sp√©cifi√©')}\n"
                f"Content: {result['document']}"
                for result in results
            ]
        )

        # Create the prompt with context
        prompt = f"""Tu es un assistant sp√©cialis√© dans l'analyse de contrats. 
Voici le contexte pertinent extrait des documents :

{context}

Question de l'utilisateur : {query}

R√©ponds de mani√®re pr√©cise en te basant uniquement sur le contexte fourni. 
Si tu ne trouves pas l'information dans le contexte, dis-le clairement."""

        # Get response from Ollama
        from rag.ollama_chat import ask_ollama

        response = ask_ollama(prompt)

    print("\nü§ñ R√©ponse :")
    print(response)

    # Display sources with metadata
    print("\nüìö Sources :")
    print("=" * 80)
    for i, result in enumerate(results, 1):
        print("\n" + "-" * 40)
        print(f"\nSource {i}/{len(results)}")
        print("-" * 40)

        print(f"Distance: {result.get('distance', 1.0):.4f}")
        if 'source' in result and result['source'] == 'graph':
            print(f"Source: D√©couvert via l'analyse graphique")

        # Afficher le contenu
        print(result["metadata"].get("content", result["document"])[:200] + "...")
        print("-" * 40)

    print(f"\nüìä Nombre total de sources: {len(results)}")


def hybrid_chunk_text(text, document_title):
    """M√©thode hybride combinant chunking intelligent et s√©mantique"""
    # 1. D√©couper d'abord selon la structure (sections principales)
    splitter = IntelligentSplitter(document_title=document_title)
    structure_chunks = splitter.split(text)

    # 2. Pour chaque grande section, appliquer le chunking s√©mantique si n√©cessaire
    final_chunks = []
    for chunk in structure_chunks:
        # Si le chunk est petit, le garder tel quel
        if len(chunk.content) < 1000:
            final_chunks.append(chunk)
        else:
            # Pour les sections longues, appliquer chunking s√©mantique
            semantic_manager = SemanticChunkManager(
                breakpoint_threshold_type="percentile",
                breakpoint_threshold_amount=0.6,
                buffer_size=2,
            )
            # Pr√©server les m√©tadonn√©es de la section dans les sous-chunks
            semantic_sub_chunks = semantic_manager.chunk_text(chunk.content)
            for sub_chunk in semantic_sub_chunks:
                sub_chunk.section_number = chunk.section_number
                sub_chunk.hierarchy = chunk.hierarchy
                sub_chunk.document_title = chunk.document_title
                sub_chunk.parent_section = chunk.parent_section
                sub_chunk.chapter_title = chunk.chapter_title
            final_chunks.extend(semantic_sub_chunks)

    return final_chunks


def calculate_optimal_threshold(text):
    """Calcule le seuil optimal bas√© sur la complexit√© du texte"""
    # Cette fonction est maintenant d√©l√©gu√©e au SemanticChunkManager qui contient
    # une impl√©mentation plus sophistiqu√©e via _calculate_optimal_threshold

    from rag.semantic_chunker import SemanticChunkManager

    # Cr√©er une instance temporaire pour utiliser la m√©thode
    temp_manager = SemanticChunkManager()

    # Utiliser la m√©thode am√©lior√©e du manager
    return temp_manager._calculate_optimal_threshold(text)


def preprocess_legal_text(text):
    """Pr√©traite le texte juridique pour pr√©server les clauses"""
    # Cette fonction est maintenant d√©l√©gu√©e au SemanticChunkManager qui contient
    # une impl√©mentation plus sophistiqu√©e via _preprocess_text_with_section_markers
    # avec un ensemble √©largi de patterns juridiques et de r√©f√©rences crois√©es

    # Nous gardons ce code pour compatibilit√©, mais il utilise maintenant les patterns
    # du SemanticChunkManager pour une coh√©rence dans le traitement

    from rag.semantic_chunker import SemanticChunkManager

    # Cr√©er une instance temporaire pour acc√©der aux patterns
    temp_manager = SemanticChunkManager()

    # Traiter le texte en utilisant les patterns du manager
    processed_lines = []
    for line in text.split("\n"):
        if any(re.search(pattern, line) for pattern in temp_manager.legal_patterns):
            processed_lines.append("[CLAUSE_START]" + line)
        elif any(
            re.search(pattern, line) for pattern in temp_manager.cross_ref_patterns
        ):
            processed_lines.append("[CROSS_REF]" + line)
        else:
            processed_lines.append(line)

    return "\n".join(processed_lines)


def restore_important_content(original_text: str, chunks: List[Chunk]) -> List[Chunk]:
    """
    Fonction de post-traitement qui identifie les lignes juridiques importantes
    qui ont √©t√© supprim√©es et les r√©int√®gre dans les chunks appropri√©s.
    NE restaure PAS les titres, mais restaure toutes les lignes de contenu juridique.
    
    Args:
        original_text: Texte original du document
        chunks: Liste des chunks apr√®s d√©coupage initial
        
    Returns:
        Liste des chunks avec le contenu important restaur√©
    """
    print("\nüîÑ Post-traitement: recherche de contenu juridique important supprim√©...")
    
    # Extraire toutes les lignes du texte original
    original_lines = []
    for line in original_text.split("\n"):
        line = line.strip()
        if line:  # Ignorer les lignes vides
            original_lines.append(line)
    
    # Extraire toutes les lignes des chunks
    chunk_lines = []
    for chunk in chunks:
        for line in chunk.content.split("\n"):
            line = line.strip()
            if line:  # Ignorer les lignes vides
                chunk_lines.append(line)
    
    # Fonction pour normaliser les lignes avant comparaison
    def normalize_line(text):
        return re.sub(r"\s+", " ", text).strip()
    
    # Identifier les lignes manquantes
    missing_lines = []
    for line in original_lines:
        normalized_line = normalize_line(line)
        found = False
        
        for chunk_line in chunk_lines:
            normalized_chunk = normalize_line(chunk_line)
            if normalized_line == normalized_chunk:
                found = True
                break
                
        if not found:
            missing_lines.append(line)
    
    # Fonction pour d√©tecter si une ligne est un titre
    def is_title(line):
        # Patterns pour identifier les titres
        title_patterns = [
            # Titres markdown (# Titre)
            r"^#+\s+\**[A-Za-z0-9]",
            # Formats explicites de sections num√©rot√©es
            r"^ARTICLE\s+[IVXLCDM]+\s*[:\.]",
            r"^SECTION\s+[0-9]+\s*[:\.]",
            r"^CHAPTER\s+[0-9]+\s*[:\.]",
            r"^ANNEXE?\s+[A-Z]:",
            r"^APPENDIX\s+[A-Z]:",
            # Titres num√©rot√©s
            r"^[0-9]+(\.[0-9]+)*\s+[A-Z]",
            # Titres avec des caract√®res sp√©ciaux
            r"^\*\*[A-Z]",
            r"^[A-Z][A-Z\s]+:$",
            # Titres courts tout en majuscules
            r"^[A-Z][A-Z\s]{1,30}$"
        ]
        
        # V√©rifier si la ligne correspond √† un des patterns de titre
        for pattern in title_patterns:
            if re.match(pattern, line):
                return True
                
        # Autres indices de titres
        if line.isupper() and len(line.split()) <= 5:
            return True
            
        return False
    
    # Crit√®res pour identifier les lignes juridiques importantes - APPROCHE TR√àS PERMISSIVE
    def is_important_legal_content(line):
        # D'abord v√©rifier si c'est un titre - si oui, ce n'est pas du contenu juridique √† restaurer
        if is_title(line):
            return False
            
        # Mots-cl√©s juridiques importants (LISTE √âTENDUE)
        legal_keywords = [
            # Termes juridiques standards
            "notwithstanding", "shall be", "exclusive remedy", 
            "sole and exclusive", "right to terminate", "limitation of liability",
            "indemnify", "warranty", "warranties", "liabilities", "liability",
            "remedies", "remedy", "disclaims", "disclaim", "claims", "claim",
            "damages", "damage", "breach", "termination", "terminate",
            "force majeure", "intellectual property", "confidential",
            "liquidated damages", "penalties", "penalty", 
            
            # Termes contractuels additionnels
            "shall", "obligation", "obligations", "responsibility", "responsibilities",
            "rights", "right", "terms", "conditions", "provisions", "stipulations",
            "agreement", "contract", "hereof", "herein", "thereof", "therein",
            "delivery", "deliver", "payment", "pay", "price", "fee", "fees",
            "delay", "delays", "timely", "schedule", "schedules", "deadline", 
            "deadline", "milestones", "milestone", "completion", "complete",
            "acceptance", "accepts", "accept", "approved", "approve", "approval",
            "rejected", "reject", "rejection", "dispute", "disputes", "resolution",
            "test", "testing", "inspection", "inspect", "audit", "review",
            "pursuant", "accordance", "compliance", "comply", "applicable",
            "indemnification", "indemnify", "indemnified", "indemnities",
            "insurance", "insured", "coverage", "purchaser", "supplier", "parties"
        ]
        
        # Expressions r√©guli√®res pour clauses sp√©cifiques
        clause_references = [
            r"clause\s+\d+(\.\d+)?", r"article\s+\d+(\.\d+)?",
            r"section\s+\d+(\.\d+)?", r"pursuant to", r"in accordance with",
            r"subject to", r"appendix [a-z]", r"annex [a-z]"
        ]
        
        # V√©rifier les mots-cl√©s
        line_lower = line.lower()
        if any(keyword in line_lower for keyword in legal_keywords):
            return True
            
        # V√©rifier les r√©f√©rences √† des clauses
        if any(re.search(pattern, line_lower) for pattern in clause_references):
            return True
            
        # Reconna√Ætre les clauses de limitation ou d'exclusion
        exclusion_patterns = [
            r"not be liable", r"no liability", r"shall not",
            r"exclude[sd]?", r"except", r"exempted", r"limitation", 
            r"limited to", r"restrict(ed|ion)", r"waive[sd]?", r"waiver"
        ]
        
        if any(re.search(pattern, line_lower) for pattern in exclusion_patterns):
            return True
            
        # Structure conditionnelle typique des clauses contractuelles
        if re.search(r"if\s+.*\s+(shall|may|must|will|is|are)\s+", line_lower):
            return True
            
        # Si la ligne contient des termes d'obligations, de conditions ou de cons√©quences
        if re.search(r"(shall|may|must|will)\s+.*\s+(if|unless|until|provided that)", line_lower):
            return True
            
        # Lignes qui commencent par des termes d'obligation contractuelle
        if re.search(r"^(the\s+)?(purchaser|supplier|contractor|client|party|parties)\s+(shall|may|will|must)", line_lower):
            return True
            
        # Lignes qui parlent de documents, livraisons, ou paiements
        if re.search(r"(documents?|delivery|payment|invoice|fee|compensation|reimbursement)", line_lower):
            return True
            
        return False
    
    # Identifier les lignes juridiques importantes parmi les lignes manquantes
    important_lines = [line for line in missing_lines if is_important_legal_content(line)]
    
    if not important_lines:
        print("‚úÖ Aucun contenu juridique important n'a √©t√© supprim√©.")
        return chunks
        
    print(f"üîç {len(important_lines)} lignes de contenu juridique important identifi√©es pour restauration.")
    
    # Fonction pour trouver le meilleur chunk pour restaurer une ligne
    def find_best_chunk(line, chunks):
        # Trouver le contexte de la ligne dans le texte original
        line_index = original_lines.index(line)
        context_before = original_lines[max(0, line_index-5):line_index]
        context_after = original_lines[line_index+1:min(len(original_lines), line_index+6)]
        
        best_chunk = None
        best_score = -1
        
        for chunk in chunks:
            score = 0
            chunk_content = chunk.content.lower()
            
            # V√©rifier si des lignes du contexte sont dans ce chunk
            for ctx_line in context_before + context_after:
                if normalize_line(ctx_line.lower()) in normalize_line(chunk_content):
                    score += 3  # Augmenter le poids du contexte
            
            # V√©rifier si le chunk contient des mots-cl√©s de la m√™me section
            line_words = set(line.lower().split())
            chunk_words = set(chunk_content.split())
            common_words = line_words.intersection(chunk_words)
            score += len(common_words) * 0.2
            
            # V√©rifier si le num√©ro de section correspond
            if hasattr(chunk, 'section_number') and chunk.section_number:
                # Extraire des num√©ros potentiels de section depuis la ligne
                section_matches = re.findall(r"clause\s+(\d+(\.\d+)?)", line.lower())
                if section_matches:
                    for match in section_matches:
                        if match[0] in chunk.section_number:
                            score += 3
            
            if score > best_score:
                best_score = score
                best_chunk = chunk
        
        # Si aucun chunk n'a un bon score, prendre celui qui a la meilleure correspondance textuelle
        if best_score <= 1:
            highest_score = -1
            best_matching_chunk = None
            
            for chunk in chunks:
                chunk_content = chunk.content.lower()
                
                # Si la ligne fait partie d'une section num√©rot√©e, essayer de trouver cette section
                section_match = re.search(r"\b(\d+(\.\d+)?)\b", line.lower())
                if section_match and section_match.group(1) in chunk_content:
                    return chunk
                
                # Sinon, utiliser la correspondance textuelle
                line_words = set(line.lower().split())
                chunk_words = set(chunk_content.split())
                common_words = line_words.intersection(chunk_words)
                
                if len(line_words) > 0:
                    score = len(common_words) / len(line_words)
                    
                    if score > highest_score:
                        highest_score = score
                        best_matching_chunk = chunk
            
            if best_matching_chunk:
                return best_matching_chunk
        
        return best_chunk
    
    # Restaurer les lignes importantes dans les chunks appropri√©s
    restored_chunks = list(chunks)  # Copie pour √©viter de modifier l'original
    for line in important_lines:
        best_chunk = find_best_chunk(line, restored_chunks)
        if best_chunk:
            # Ajouter la ligne au chunk (√† la fin)
            best_chunk.content = best_chunk.content + "\n\n" + line
            print(f"‚úÖ Ligne restaur√©e dans un chunk appropri√©: {line[:60]}...")
        else:
            # Si aucun chunk appropri√© n'est trouv√©, cr√©er un nouveau chunk
            print(f"‚ö†Ô∏è Cr√©ation d'un nouveau chunk pour la ligne: {line[:60]}...")
            new_chunk = Chunk(
                content=line, 
                section_number="unknown",
                hierarchy=["restored_content"],
                document_title=chunks[0].document_title if chunks else "unknown",
                parent_section="Restored Content",
                chapter_title="Restored Legal Content"
            )
            restored_chunks.append(new_chunk)
    
    print("‚úÖ Post-traitement termin√©. Contenu juridique important restaur√©.")
    return restored_chunks


if __name__ == "__main__":
    # Check command line arguments
    if len(sys.argv) < 2:
        print(
            "Usage: python main.py <contract_file> [--semantic-chunking] [search_query|--chat|--graphchat]"
        )
        sys.exit(1)

    filepath = sys.argv[1]
    
    # Check for semantic chunking flag
    use_semantic_chunking = "--semantic-chunking" in sys.argv
    if use_semantic_chunking:
        sys.argv.remove("--semantic-chunking")

    # If --chat is provided, enter chat mode
    if "--chat" in sys.argv:
        print("\nüí¨ Mode chat activ√©. Tapez 'exit' pour quitter.")
        while True:
            query = input("\nVotre question : ")
            if query.lower() == "exit":
                break
            chat_with_contract(query, use_graph=False)
    # If --graphchat is provided, enter graph-enhanced chat mode
    elif "--graphchat" in sys.argv:
        print("\nüìä Mode GraphRAG chat activ√©. Tapez 'exit' pour quitter.")
        max_hop = 2  # Default value
        for i, arg in enumerate(sys.argv):
            if arg == "--max-hop" and i + 1 < len(sys.argv):
                try:
                    max_hop = int(sys.argv[i + 1])
                except ValueError:
                    pass
        
        while True:
            query = input("\nVotre question : ")
            if query.lower() == "exit":
                break
            chat_with_contract(query, use_graph=True, max_hop=max_hop)
    # If search query is provided, perform search
    elif len(sys.argv) > 2:
        use_graph = "--graph" in sys.argv
        if use_graph and "--graph" in sys.argv:
            sys.argv.remove("--graph")
        
        search_query = " ".join(sys.argv[2:])
        search_contracts(search_query, use_graph=use_graph)
    else:
        # Process the contract
        chunks = process_contract(filepath)
